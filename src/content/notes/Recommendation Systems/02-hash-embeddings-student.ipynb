{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Hash Embeddings - Student Version\n",
    "\n",
    "> **TODO**: Fill in the missing code implementations!\n",
    "\n",
    "This notebook teaches hash embeddings for billion-scale vocabularies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "EMB_SIZE = 64\n",
    "NUM_USER_HASHES = 2\n",
    "NUM_ITEM_HASHES = 2\n",
    "NUM_AUTHOR_HASHES = 2\n",
    "VOCAB_SIZE = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: MultiHashEmbedding\n",
    "\n",
    "**Task**: Implement multi-hash embedding lookup.\n",
    "\n",
    "**Hint**: Use `torch.randn` for initialization and index-based lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHashEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-hash embedding with multiple hash tables.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of entity vocabulary\n",
    "        emb_size: Embedding dimension\n",
    "        num_hashes: Number of hash functions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size, num_hashes):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize embedding tables (one per hash function)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, hash_indices):\n",
    "        \"\"\"\n",
    "        Look up embeddings for given hash indices.\n",
    "        \n",
    "        Args:\n",
    "            hash_indices: [B, num_hashes] tensor\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: [B, num_hashes, emb_size] tensor\n",
    "        \"\"\"\n",
    "        # TODO: Look up embeddings from each hash table\n",
    "        pass\n",
    "\n",
    "# TEST: MultiHashEmbedding\n",
    "def test_multi_hash_embedding():\n",
    "    hash_emb = MultiHashEmbedding(VOCAB_SIZE, EMB_SIZE, NUM_USER_HASHES)\n",
    "    \n",
    "    user_hashes = torch.randint(1, VOCAB_SIZE, (4, NUM_USER_HASHES))\n",
    "    embeddings = hash_emb(user_hashes)\n",
    "    \n",
    "    # Assertions\n",
    "    assert embeddings.shape == torch.Size([4, NUM_USER_HASHES, EMB_SIZE]), \\\n",
    "        f\"Expected shape [4, {NUM_USER_HASHES}, {EMB_SIZE}], got {embeddings.shape}\"\n",
    "    \n",
    "    print(\"Shape test passed!\")\n",
    "    return True\n",
    "\n",
    "test_multi_hash_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: BlockUserReduce\n",
    "\n",
    "**Task**: Flatten and project user hash embeddings into a single user token.\n",
    "\n",
    "**Hint**: Reshape → Linear project → Unsqueeze for sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockUserReduce(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine multiple user hash embeddings into a single user representation.\n",
    "    \n",
    "    Args:\n",
    "        num_hashes: Number of hash functions\n",
    "        emb_size: Embedding dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_hashes, emb_size):\n",
    "        super().__init__()\n",
    "        # TODO: Create projection layer: num_hashes * emb_size -> emb_size\n",
    "        pass\n",
    "    \n",
    "    def forward(self, user_hashes, user_embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_hashes: [B, num_hashes] (0 = padding)\n",
    "            user_embeddings: [B, num_hashes, emb_size]\n",
    "        \n",
    "        Returns:\n",
    "            user_embedding: [B, 1, emb_size]\n",
    "            user_padding_mask: [B, 1]\n",
    "        \"\"\"\n",
    "        # TODO: Flatten → Project → Add sequence dim → Create padding mask\n",
    "        pass\n",
    "\n",
    "# TEST: BlockUserReduce\n",
    "def test_block_user_reduce():\n",
    "    block_user = BlockUserReduce(NUM_USER_HASHES, EMB_SIZE)\n",
    "    \n",
    "    user_hashes = torch.randint(0, VOCAB_SIZE, (4, NUM_USER_HASHES))\n",
    "    user_embeddings = torch.randn(4, NUM_USER_HASHES, EMB_SIZE)\n",
    "    \n",
    "    user_emb, user_mask = block_user(user_hashes, user_embeddings)\n",
    "    \n",
    "    # Assertions\n",
    "    assert user_emb.shape == torch.Size([4, 1, EMB_SIZE]), \\\n",
    "        f\"Expected [4, 1, {EMB_SIZE}], got {user_emb.shape}\"\n",
    "    assert user_mask.shape == torch.Size([4, 1])\n",
    "    \n",
    "    print(\"Shape tests passed!\")\n",
    "    return True\n",
    "\n",
    "test_block_user_reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: BlockHistoryReduce\n",
    "\n",
    "**Task**: Combine 4 ingredients (post, author, actions, surface) into history tokens.\n",
    "\n",
    "**Hint**: Concatenate all ingredients → Linear project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockHistoryReduce(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine history ingredients into sequence embeddings.\n",
    "    \n",
    "    Args:\n",
    "        num_item_hashes: Hashes per item\n",
    "        num_author_hashes: Hashes per author\n",
    "        emb_size: Output dimension\n",
    "        num_actions: Number of action types\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_item_hashes, num_author_hashes, emb_size, num_actions=19):\n",
    "        super().__init__()\n",
    "        # TODO: Create action projection, surface embedding, and main projection\n",
    "        pass\n",
    "    \n",
    "    def forward(self, post_emb, author_emb, actions, surface):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            post_emb: [B, S, num_item_hashes, emb_size]\n",
    "            author_emb: [B, S, num_author_hashes, emb_size]\n",
    "            actions: [B, S, num_actions] (multi-hot)\n",
    "            surface: [B, S]\n",
    "        \n",
    "        Returns:\n",
    "            history_emb: [B, S, emb_size]\n",
    "            history_mask: [B, S]\n",
    "        \"\"\"\n",
    "        # TODO: Flatten → Project actions → Embed surface → Concat all → Project\n",
    "        pass\n",
    "\n",
    "# TEST: BlockHistoryReduce\n",
    "def test_block_history_reduce():\n",
    "    block_history = BlockHistoryReduce(NUM_ITEM_HASHES, NUM_AUTHOR_HASHES, EMB_SIZE)\n",
    "    \n",
    "    B, S = 2, 5\n",
    "    post_emb = torch.randn(B, S, NUM_ITEM_HASHES, EMB_SIZE)\n",
    "    author_emb = torch.randn(B, S, NUM_AUTHOR_HASHES, EMB_SIZE)\n",
    "    actions = torch.zeros(B, S, 19)\n",
    "    actions[0, 0, [0, 1]] = 1\n",
    "    surface = torch.randint(0, 16, (B, S))\n",
    "    \n",
    "    history_emb, history_mask = block_history(post_emb, author_emb, actions, surface)\n",
    "    \n",
    "    # Assertions\n",
    "    assert history_emb.shape == torch.Size([B, S, EMB_SIZE]), \\\n",
    "        f\"Expected [{B}, {S}, {EMB_SIZE}], got {history_emb.shape}\"\n",
    "    assert history_mask.shape == torch.Size([B, S])\n",
    "    \n",
    "    print(\"Shape tests passed!\")\n",
    "    return True\n",
    "\n",
    "test_block_history_reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Exercises completed**:\n",
    "- [ ] MultiHashEmbedding\n",
    "- [ ] BlockUserReduce\n",
    "- [ ] BlockHistoryReduce\n",
    "\n",
    "**Run all tests** to verify your implementations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
