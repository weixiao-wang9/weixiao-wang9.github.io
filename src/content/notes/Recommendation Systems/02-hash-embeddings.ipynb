{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Hash Embeddings - Interactive Notebook\n",
    "\n",
    "> **Companion to**: 02-hash-embeddings.md\n",
    "> **Run time**: ~5 minutes\n",
    "\n",
    "This notebook implements the hash embedding system from X's recommendation algorithm.\n",
    "\n",
    "**Key concepts covered:**\n",
    "- Multi-hash collision avoidance\n",
    "- Block user/history/candidate reduction\n",
    "- RecsysBatch/RecsysEmbeddings data containers\n",
    "- Padding masks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration (matches X's HashConfig from recsys_model.py)\n",
    "EMB_SIZE = 64\n",
    "NUM_USER_HASHES = 2\n",
    "NUM_ITEM_HASHES = 2\n",
    "NUM_AUTHOR_HASHES = 2\n",
    "VOCAB_SIZE = 100000  # Simulating 100K entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MultiHashEmbedding Module\n",
    "\n",
    "Simulates hash tables with lookup. In production, X uses actual hash functions to map entity IDs to embedding table indices. Here we simulate with random projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHashEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-hash embedding table that reduces collisions.\n",
    "    \n",
    "    Instead of one embedding table, we use NUM_HASHES separate tables.\n",
    "    Each entity gets embeddings from each table, then we combine them.\n",
    "    \n",
    "    This reduces collision probability from 1/vocab_size to 1/vocab_size^NUM_HASHES.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, emb_size: int, num_hashes: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.num_hashes = num_hashes\n",
    "        \n",
    "        # Create separate embedding tables for each hash function\n",
    "        # Shape: [num_hashes, vocab_size, emb_size]\n",
    "        self.embeddings = nn.Parameter(\n",
    "            torch.randn(num_hashes, vocab_size, emb_size) * 0.1\n",
    "        )\n",
    "    \n",
    "    def forward(self, hash_indices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Look up embeddings for given hash indices.\n",
    "        \n",
    "        Args:\n",
    "            hash_indices: [B, num_hashes] tensor of hash indices\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: [B, num_hashes, emb_size] tensor of looked-up embeddings\n",
    "        \"\"\"\n",
    "        B, num_hashes = hash_indices.shape\n",
    "        assert num_hashes == self.num_hashes, \\\n",
    "            f\"Expected {self.num_hashes} hash indices, got {num_hashes}\"\n",
    "        \n",
    "        # Look up embeddings from each hash table\n",
    "        # We iterate over hash functions and gather the corresponding embeddings\n",
    "        all_embeddings = []\n",
    "        for h in range(self.num_hashes):\n",
    "            # Get indices for this hash function\n",
    "            indices = hash_indices[:, h]  # [B]\n",
    "            # Look up: [B, emb_size]\n",
    "            emb = self.embeddings[h, indices, :]\n",
    "            all_embeddings.append(emb)\n",
    "        \n",
    "        # Stack: [B, num_hashes, emb_size]\n",
    "        return torch.stack(all_embeddings, dim=1)\n",
    "\n",
    "# Test the module\n",
    "print(\"Testing MultiHashEmbedding...\")\n",
    "hash_emb = MultiHashEmbedding(VOCAB_SIZE, EMB_SIZE, NUM_USER_HASHES).to(device)\n",
    "\n",
    "# Create dummy hash indices (e.g., user hashes)\n",
    "user_hashes = torch.randint(1, VOCAB_SIZE, (4, NUM_USER_HASHES)).to(device)\n",
    "print(f\"User hashes shape: {user_hashes.shape}\")  # [4, 2]\n",
    "\n",
    "# Look up embeddings\n",
    "user_embeddings = hash_emb(user_hashes)\n",
    "print(f\"User embeddings shape: {user_embeddings.shape}\")  # [4, 2, 64]\n",
    "print(f\"✓ MultiHashEmbedding works!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BlockUserReduce Module\n",
    "\n",
    "Flattens and projects multiple user hash embeddings into a single user representation.\n",
    "\n",
    "**From recsys_model.py:79-119**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockUserReduce(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine multiple user hash embeddings into a single user representation.\n",
    "    \n",
    "    Process:\n",
    "    1. Flatten: [B, num_hashes, D] -> [B, num_hashes * D]\n",
    "    2. Project: [B, num_hashes * D] -> [B, 1, D] via linear projection\n",
    "    3. Create padding mask based on first hash (hash value 0 = padding)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_hashes: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self.num_hashes = num_hashes\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        # Projection matrix: [num_hashes * D, D]\n",
    "        self.projection = nn.Linear(num_hashes * emb_size, emb_size, bias=False)\n",
    "    \n",
    "    def forward(self, \n",
    "                user_hashes: torch.Tensor,\n",
    "                user_embeddings: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_hashes: [B, num_hashes] hash indices (0 = padding)\n",
    "            user_embeddings: [B, num_hashes, D] looked-up embeddings\n",
    "        \n",
    "        Returns:\n",
    "            user_embedding: [B, 1, D] combined user representation\n",
    "            user_padding_mask: [B, 1] boolean mask (True = valid)\n",
    "        \"\"\"\n",
    "        B = user_embeddings.shape[0]\n",
    "        \n",
    "        # Step 1: Flatten\n",
    "        # [B, num_hashes, D] -> [B, num_hashes * D]\n",
    "        user_flat = user_embeddings.reshape(B, -1)\n",
    "        \n",
    "        # Step 2: Project\n",
    "        # [B, num_hashes * D] -> [B, D]\n",
    "        user_proj = self.projection(user_flat)\n",
    "        \n",
    "        # [B, D] -> [B, 1, D]\n",
    "        user_embedding = user_proj.unsqueeze(1)\n",
    "        \n",
    "        # Step 3: Create padding mask\n",
    "        # Hash 0 is reserved for padding\n",
    "        user_padding_mask = (user_hashes[:, 0] != 0).unsqueeze(1)  # [B, 1]\n",
    "        \n",
    "        return user_embedding, user_padding_mask\n",
    "\n",
    "# Test the module\n",
    "print(\"Testing BlockUserReduce...\")\n",
    "block_user_reduce = BlockUserReduce(NUM_USER_HASHES, EMB_SIZE).to(device)\n",
    "\n",
    "# Create test data\n",
    "user_hashes = torch.randint(1, VOCAB_SIZE, (4, NUM_USER_HASHES)).to(device)\n",
    "user_embeddings = hash_emb(user_hashes)\n",
    "\n",
    "# Apply block user reduce\n",
    "user_emb, user_mask = block_user_reduce(user_hashes, user_embeddings)\n",
    "\n",
    "print(f\"Input user hashes shape: {user_hashes.shape}\")  # [4, 2]\n",
    "print(f\"Input user embeddings shape: {user_embeddings.shape}\")  # [4, 2, 64]\n",
    "print(f\"Output user embedding shape: {user_emb.shape}\")  # [4, 1, 64]\n",
    "print(f\"Output user padding mask shape: {user_mask.shape}\")  # [4, 1]\n",
    "print(f\"User padding mask: {user_mask.flatten()}\")\n",
    "print(f\"✓ BlockUserReduce works!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BlockHistoryReduce Module\n",
    "\n",
    "Combines 4 ingredients for each history item into a single embedding:\n",
    "1. Post embeddings (from hash)\n",
    "2. Author embeddings (from hash)\n",
    "3. Product surface embeddings (learned lookup)\n",
    "4. Action embeddings (multi-hot)\n",
    "\n",
    "**From recsys_model.py:122-182**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockHistoryReduce(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine history embeddings (post, author, actions, product_surface) into sequence.\n",
    "    \n",
    "    For each history position:\n",
    "    1. Flatten post embeddings: [B, S, num_item_hashes, D] -> [B, S, num_item_hashes * D]\n",
    "    2. Flatten author embeddings: [B, S, num_author_hashes, D] -> [B, S, num_author_hashes * D]\n",
    "    3. Concatenate: post + author + actions + product_surface\n",
    "    4. Project to single embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_item_hashes: int,\n",
    "                 num_author_hashes: int,\n",
    "                 emb_size: int,\n",
    "                 num_actions: int = 19):\n",
    "        super().__init__()\n",
    "        self.num_item_hashes = num_item_hashes\n",
    "        self.num_author_hashes = num_author_hashes\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        # Input dimension for projection\n",
    "        input_dim = (\n",
    "            num_item_hashes * emb_size +  # Post embeddings\n",
    "            num_author_hashes * emb_size +  # Author embeddings\n",
    "            emb_size +  # Action embeddings\n",
    "            emb_size  # Product surface embeddings\n",
    "        )\n",
    "        \n",
    "        self.projection = nn.Linear(input_dim, emb_size, bias=False)\n",
    "        \n",
    "        # Product surface embedding table (learned)\n",
    "        self.product_surface_emb = nn.Embedding(16, emb_size)  # 16 surface types\n",
    "        \n",
    "        # Action embedding projection (for multi-hot actions)\n",
    "        self.action_proj = nn.Linear(num_actions, emb_size, bias=False)\n",
    "    \n",
    "    def forward(self,\n",
    "                history_post_hashes: torch.Tensor,\n",
    "                history_post_embeddings: torch.Tensor,\n",
    "                history_author_embeddings: torch.Tensor,\n",
    "                history_product_surface: torch.Tensor,\n",
    "                history_actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            history_post_hashes: [B, S, num_item_hashes] hash indices\n",
    "            history_post_embeddings: [B, S, num_item_hashes, D] post embeddings\n",
    "            history_author_embeddings: [B, S, num_author_hashes, D] author embeddings\n",
    "            history_product_surface: [B, S] product surface indices\n",
    "            history_actions: [B, S, num_actions] multi-hot action vectors\n",
    "        \n",
    "        Returns:\n",
    "            history_embeddings: [B, S, D] combined history sequence\n",
    "            history_padding_mask: [B, S] boolean mask (True = valid)\n",
    "        \"\"\"\n",
    "        B, S, _, D = history_post_embeddings.shape\n",
    "        \n",
    "        # Step 1: Flatten post embeddings\n",
    "        # [B, S, num_item_hashes, D] -> [B, S, num_item_hashes * D]\n",
    "        post_flat = history_post_embeddings.reshape(B, S, -1)\n",
    "        \n",
    "        # Step 2: Flatten author embeddings\n",
    "        # [B, S, num_author_hashes, D] -> [B, S, num_author_hashes * D]\n",
    "        author_flat = history_author_embeddings.reshape(B, S, -1)\n",
    "        \n",
    "        # Step 3: Embed actions (signed multi-hot trick)\n",
    "        # 2 * actions - 1 maps {0,1} to {-1, +1}\n",
    "        actions_signed = (2 * history_actions - 1).float()  # [B, S, num_actions]\n",
    "        action_emb = self.action_proj(actions_signed)  # [B, S, D]\n",
    "        \n",
    "        # Mask out invalid positions (no actions)\n",
    "        valid_mask = (history_actions.sum(dim=-1) > 0).unsqueeze(-1)  # [B, S, 1]\n",
    "        action_emb = action_emb * valid_mask\n",
    "        \n",
    "        # Step 4: Product surface embeddings\n",
    "        surface_emb = self.product_surface_emb(history_product_surface)  # [B, S, D]\n",
    "        \n",
    "        # Step 5: Concatenate all ingredients\n",
    "        combined = torch.cat([post_flat, author_flat, action_emb, surface_emb], dim=-1)\n",
    "        # [B, S, num_item_hashes*D + num_author_hashes*D + D + D]\n",
    "        \n",
    "        # Step 6: Project to single embedding\n",
    "        history_emb = self.projection(combined)  # [B, S, D]\n",
    "        \n",
    "        # Step 7: Create padding mask (based on first post hash)\n",
    "        history_padding_mask = (history_post_hashes[:, :, 0] != 0)  # [B, S]\n",
    "        \n",
    "        return history_emb, history_padding_mask\n",
    "\n",
    "# Test the module\n",
    "print(\"Testing BlockHistoryReduce...\")\n",
    "block_history_reduce = BlockHistoryReduce(\n",
    "    NUM_ITEM_HASHES, NUM_AUTHOR_HASHES, EMB_SIZE, num_actions=19\n",
    ").to(device)\n",
    "\n",
    "# Create test history data\n",
    "B, S = 4, 10  # Batch size 4, sequence length 10\n",
    "history_post_hashes = torch.randint(0, VOCAB_SIZE, (B, S, NUM_ITEM_HASHES)).to(device)\n",
    "history_post_hashes[:, S//2:, :] = 0  # Zero out second half (padding)\n",
    "\n",
    "history_post_embeddings = torch.randn(B, S, NUM_ITEM_HASHES, EMB_SIZE).to(device)\n",
    "history_author_embeddings = torch.randn(B, S, NUM_AUTHOR_HASHES, EMB_SIZE).to(device)\n",
    "history_product_surface = torch.randint(0, 16, (B, S)).to(device)\n",
    "history_actions = torch.zeros(B, S, 19).to(device)\n",
    "history_actions[:, :S//2, :] = torch.randint(0, 2, (B, S//2, 19)).to(device)  # First half valid\n",
    "\n",
    "# Apply block history reduce\n",
    "history_emb, history_mask = block_history_reduce(\n",
    "    history_post_hashes,\n",
    "    history_post_embeddings,\n",
    "    history_author_embeddings,\n",
    "    history_product_surface,\n",
    "    history_actions\n",
    ")\n",
    "\n",
    "print(f\"History sequence length: {S}\")\n",
    "print(f\"Output history embeddings shape: {history_emb.shape}\")  # [4, 10, 64]\n",
    "print(f\"Output history padding mask shape: {history_mask.shape}\")  # [4, 10]\n",
    "print(f\"Valid positions per sample: {history_mask.sum(dim=1).tolist()}\")\n",
    "print(f\"✓ BlockHistoryReduce works!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RecsysBatch and RecsysEmbeddings Data Containers\n",
    "\n",
    "Data structures that hold the input features and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RecsysBatch:\n",
    "    \"\"\"\n",
    "    Input batch for the recommendation model.\n",
    "    \n",
    "    Contains the feature data (hashes, actions, product surfaces) but NOT the embeddings.\n",
    "    Embeddings are passed separately via RecsysEmbeddings.\n",
    "    \"\"\"\n",
    "    user_hashes: torch.Tensor              # [B, num_user_hashes]\n",
    "    history_post_hashes: torch.Tensor      # [B, S, num_item_hashes]\n",
    "    history_author_hashes: torch.Tensor    # [B, S, num_author_hashes]\n",
    "    history_actions: torch.Tensor          # [B, S, num_actions]\n",
    "    history_product_surface: torch.Tensor  # [B, S]\n",
    "    candidate_post_hashes: torch.Tensor    # [B, C, num_item_hashes]\n",
    "    candidate_author_hashes: torch.Tensor  # [B, C, num_author_hashes]\n",
    "    candidate_product_surface: torch.Tensor # [B, C]\n",
    "\n",
    "@dataclass\n",
    "class RecsysEmbeddings:\n",
    "    \"\"\"\n",
    "    Container for pre-looked-up embeddings from the embedding tables.\n",
    "    \n",
    "    These embeddings are looked up from hash tables before being passed to the model.\n",
    "    The block_*_reduce functions will combine multiple hash embeddings into single representations.\n",
    "    \"\"\"\n",
    "    user_embeddings: torch.Tensor          # [B, num_user_hashes, D]\n",
    "    history_post_embeddings: torch.Tensor  # [B, S, num_item_hashes, D]\n",
    "    history_author_embeddings: torch.Tensor # [B, S, num_author_hashes, D]\n",
    "    candidate_post_embeddings: torch.Tensor # [B, C, num_item_hashes, D]\n",
    "    candidate_author_embeddings: torch.Tensor # [B, C, num_author_hashes, D]\n",
    "\n",
    "print(\"Data containers defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demo: End-to-End Hash Embedding Pipeline\n",
    "\n",
    "Create synthetic data and run through the hash embedding system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEMO: Hash Embedding Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration\n",
    "batch_size = 2\n",
    "history_len = 5\n",
    "num_candidates = 3\n",
    "\n",
    "# Create hash embedding tables\n",
    "user_hash_table = MultiHashEmbedding(VOCAB_SIZE, EMB_SIZE, NUM_USER_HASHES).to(device)\n",
    "post_hash_table = MultiHashEmbedding(VOCAB_SIZE, EMB_SIZE, NUM_ITEM_HASHES).to(device)\n",
    "author_hash_table = MultiHashEmbedding(VOCAB_SIZE, EMB_SIZE, NUM_AUTHOR_HASHES).to(device)\n",
    "\n",
    "# Create block reducers\n",
    "block_user = BlockUserReduce(NUM_USER_HASHES, EMB_SIZE).to(device)\n",
    "block_history = BlockHistoryReduce(NUM_ITEM_HASHES, NUM_AUTHOR_HASHES, EMB_SIZE).to(device)\n",
    "\n",
    "# Generate synthetic hash indices\n",
    "user_hashes = torch.randint(1, VOCAB_SIZE, (batch_size, NUM_USER_HASHES)).to(device)\n",
    "history_post_hashes = torch.randint(1, VOCAB_SIZE, (batch_size, history_len, NUM_ITEM_HASHES)).to(device)\n",
    "history_author_hashes = torch.randint(1, VOCAB_SIZE, (batch_size, history_len, NUM_AUTHOR_HASHES)).to(device)\n",
    "candidate_post_hashes = torch.randint(1, VOCAB_SIZE, (batch_size, num_candidates, NUM_ITEM_HASHES)).to(device)\n",
    "candidate_author_hashes = torch.randint(1, VOCAB_SIZE, (batch_size, num_candidates, NUM_AUTHOR_HASHES)).to(device)\n",
    "\n",
    "# Other features\n",
    "history_product_surface = torch.randint(0, 16, (batch_size, history_len)).to(device)\n",
    "history_actions = torch.randint(0, 2, (batch_size, history_len, 19)).float().to(device)\n",
    "candidate_product_surface = torch.randint(0, 16, (batch_size, num_candidates)).to(device)\n",
    "\n",
    "print(f\"\\n1. Input shapes:\")\n",
    "print(f\"   User hashes: {user_hashes.shape}\")\n",
    "print(f\"   History post hashes: {history_post_hashes.shape}\")\n",
    "print(f\"   History author hashes: {history_author_hashes.shape}\")\n",
    "print(f\"   Candidate post hashes: {candidate_post_hashes.shape}\")\n",
    "print(f\"   Candidate author hashes: {candidate_author_hashes.shape}\")\n",
    "\n",
    "# Step 1: Look up embeddings from hash tables\n",
    "print(f\"\\n2. Looking up embeddings from hash tables...\")\n",
    "user_embeddings = user_hash_table(user_hashes)  # [B, num_user_hashes, D]\n",
    "history_post_embeddings = post_hash_table(\n",
    "    history_post_hashes.reshape(-1, NUM_ITEM_HASHES)\n",
    ").reshape(batch_size, history_len, NUM_ITEM_HASHES, EMB_SIZE)\n",
    "history_author_embeddings = author_hash_table(\n",
    "    history_author_hashes.reshape(-1, NUM_AUTHOR_HASHES)\n",
    ").reshape(batch_size, history_len, NUM_AUTHOR_HASHES, EMB_SIZE)\n",
    "candidate_post_embeddings = post_hash_table(\n",
    "    candidate_post_hashes.reshape(-1, NUM_ITEM_HASHES)\n",
    ").reshape(batch_size, num_candidates, NUM_ITEM_HASHES, EMB_SIZE)\n",
    "candidate_author_embeddings = author_hash_table(\n",
    "    candidate_author_hashes.reshape(-1, NUM_AUTHOR_HASHES)\n",
    ").reshape(batch_size, num_candidates, NUM_AUTHOR_HASHES, EMB_SIZE)\n",
    "\n",
    "print(f\"   User embeddings: {user_embeddings.shape}\")\n",
    "print(f\"   History post embeddings: {history_post_embeddings.shape}\")\n",
    "print(f\"   History author embeddings: {history_author_embeddings.shape}\")\n",
    "print(f\"   Candidate post embeddings: {candidate_post_embeddings.shape}\")\n",
    "print(f\"   Candidate author embeddings: {candidate_author_embeddings.shape}\")\n",
    "\n",
    "# Step 2: Reduce embeddings\n",
    "print(f\"\\n3. Applying block reduction...\")\n",
    "user_emb, user_mask = block_user(user_hashes, user_embeddings)\n",
    "history_emb, history_mask = block_history(\n",
    "    history_post_hashes,\n",
    "    history_post_embeddings,\n",
    "    history_author_embeddings,\n",
    "    history_product_surface,\n",
    "    history_actions\n",
    ")\n",
    "\n",
    "print(f\"   User representation: {user_emb.shape}\")\n",
    "print(f\"   User padding mask: {user_mask.shape}\")\n",
    "print(f\"   History representation: {history_emb.shape}\")\n",
    "print(f\"   History padding mask: {history_mask.shape}\")\n",
    "\n",
    "# Step 3: Verify shapes\n",
    "assert user_emb.shape == (batch_size, 1, EMB_SIZE)\n",
    "assert user_mask.shape == (batch_size, 1)\n",
    "assert history_emb.shape == (batch_size, history_len, EMB_SIZE)\n",
    "assert history_mask.shape == (batch_size, history_len)\n",
    "\n",
    "print(f\"\\n✓ All shape assertions passed!\")\n",
    "print(f\"\\nFinal output:\")\n",
    "print(f\"  User token: {user_emb.shape} (ready for transformer)\")\n",
    "print(f\"  History sequence: {history_emb.shape} (ready for transformer)\")\n",
    "print(f\"\\nThese would be concatenated and fed into the transformer!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization: Collision Probability vs Number of Hashes\n",
    "\n",
    "Show how using multiple hash functions reduces collision probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute collision probability\n",
    "vocab_sizes = [1000, 10000, 100000, 1000000]\n",
    "num_hashes_range = range(1, 6)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    collision_probs = []\n",
    "    for n in num_hashes_range:\n",
    "        # Probability of collision = (1/vocab_size)^n\n",
    "        prob = (1.0 / vocab_size) ** n\n",
    "        collision_probs.append(prob)\n",
    "    \n",
    "    # Plot (on log scale)\n",
    "    plt.plot(list(num_hashes_range), collision_probs, \n",
    "             marker='o', label=f'Vocab={vocab_size:,}')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of Hash Functions', fontsize=12)\n",
    "plt.ylabel('Collision Probability (log scale)', fontsize=12)\n",
    "plt.title('Hash Collision Probability vs Number of Hash Functions\\n(More hashes = exponentially fewer collisions)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: With just 2 hash functions, collision probability drops by VOCAB_SIZE^2!\")\n",
    "print(\"Example: For vocab_size=100,000:\")\n",
    "print(f\"  1 hash: P(collision) = {1/100000:.6f}\")\n",
    "print(f\"  2 hashes: P(collision) = {(1/100000)**2:.12f}\")\n",
    "print(f\"  3 hashes: P(collision) = {(1/100000)**3:.18f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've implemented the hash embedding system from X's recommendation algorithm!\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **Multi-hash design**: Use multiple hash functions to dramatically reduce collision probability\n",
    "\n",
    "2. **Block reduction**: Flatten + project to combine multiple embeddings into one:\n",
    "   - `BlockUserReduce`: User hashes → single user token\n",
    "   - `BlockHistoryReduce`: 4 ingredients → single history token per position\n",
    "\n",
    "3. **Padding masks**: Track which positions are valid (hash != 0) for transformer attention\n",
    "\n",
    "4. **Scalability**: Hash embeddings allow handling billion-entity vocabularies without a massive embedding table\n",
    "\n",
    "**Next up**: [Lecture 3 - Two-Tower Retrieval](03-two-tower-retrieval.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
