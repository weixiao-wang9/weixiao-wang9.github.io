{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 Notebook: Grok Transformer\n",
    "\n",
    "**Goal**: Implement the Grok transformer building blocks in detail.\n",
    "\n",
    "You'll learn:\n",
    "- RMSNorm (replaces LayerNorm)\n",
    "- Rotary Embedding (RoPE)\n",
    "- Multi-Head Attention with GQA, RoPE, and soft capping\n",
    "- SwiGLU FFN (DenseBlock)\n",
    "- DecoderLayer with residual connections\n",
    "- Transformer stack\n",
    "- make_recsys_attn_mask for candidate isolation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import PyTorch and set up configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "EMB_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "NUM_Q_HEADS = 4\n",
    "NUM_KV_HEADS = 2\n",
    "KEY_SIZE = 16\n",
    "\n",
    "print(f\"Embedding dimension: {EMB_SIZE}\")\n",
    "print(f\"Number of layers: {NUM_LAYERS}\")\n",
    "print(f\"Number of Q heads: {NUM_Q_HEADS}\")\n",
    "print(f\"Number of KV heads: {NUM_KV_HEADS}\")\n",
    "print(f\"Key size: {KEY_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSNorm\n",
    "\n",
    "Root Mean Square Layer Normalization.\n",
    "\n",
    "**Key difference from LayerNorm**:\n",
    "- LayerNorm: Centers the data first (subtract mean), then scales\n",
    "- RMSNorm: Only scales, no centering\n",
    "\n",
    "**Why RMSNorm?**\n",
    "- Simpler (no mean computation)\n",
    "- Works just as well in practice\n",
    "- Faster (one less pass over the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization.\n",
    "    \n",
    "    RMSNorm(x) = x * rsqrt(mean(x^2) + eps) * scale\n",
    "    \n",
    "    Args:\n",
    "        dim: Feature dimension\n",
    "        eps: Numerical stability constant\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Learnable scale parameter\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, ..., D] input tensor of any shape\n",
    "        \n",
    "        Returns:\n",
    "            Normalized tensor of same shape\n",
    "        \"\"\"\n",
    "        # Compute RMS: sqrt(mean(x^2))\n",
    "        rms = torch.sqrt(torch.mean(x.float() ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        \n",
    "        # Normalize and apply scale\n",
    "        return x / rms * self.scale\n",
    "\n",
    "# Test RMSNorm\n",
    "print(\"Testing RMSNorm...\")\n",
    "norm = RMSNorm(EMB_SIZE)\n",
    "\n",
    "# Test with different shapes\n",
    "x1 = torch.randn(4, 10, EMB_SIZE)  # [B, T, D]\n",
    "x2 = torch.randn(4, EMB_SIZE)      # [B, D]\n",
    "x3 = torch.randn(EMB_SIZE)         # [D]\n",
    "\n",
    "y1 = norm(x1)\n",
    "y2 = norm(x2)\n",
    "\n",
    "print(f\"Input shape 1: {x1.shape} -> Output shape: {y1.shape}\")\n",
    "print(f\"Input shape 2: {x2.shape} -> Output shape: {y2.shape}\")\n",
    "\n",
    "# Verify RMS is ~1.0 after normalization\n",
    "rms_after = torch.sqrt(torch.mean(y1.float() ** 2, dim=-1))\n",
    "print(f\"\\nRMS after normalization (should be ~1.0): {rms_after.mean():.6f}\")\n",
    "\n",
    "assert y1.shape == x1.shape\n",
    "assert y2.shape == x2.shape\n",
    "print(\"✓ RMSNorm working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotary Embedding (RoPE)\n",
    "\n",
    "**What is RoPE?**\n",
    "- Encodes positions using rotation matrices\n",
    "- Allows the model to understand **relative positions**\n",
    "- No absolute position embeddings needed\n",
    "\n",
    "**How it works:**\n",
    "1. Split embedding into pairs of dimensions\n",
    "2. Rotate each pair by an angle proportional to position\n",
    "3. The rotation encodes relative distance\n",
    "\n",
    "Reference: https://arxiv.org/abs/2104.09864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embedding (RoPE).\n",
    "    \n",
    "    Encodes positions using rotation matrices.\n",
    "    \n",
    "    For position p and dimension d (d must be even):\n",
    "    - Embedding dimensions [d-2, d-1] are rotated by angle = p * theta_d\n",
    "    - where theta_d = base^(-2*d / dim)\n",
    "    \n",
    "    Args:\n",
    "        dim: Embedding dimension (must be even)\n",
    "        base: Base for frequency computation (default: 10000)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        \n",
    "        # Compute frequency for each dimension pair\n",
    "        # theta_d = base^(-2*d / dim)\n",
    "        assert dim % 2 == 0, \"RoPE dim must be even\"\n",
    "        freqs = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('freqs', freqs)\n",
    "    \n",
    "    def rotate_half(self, x):\n",
    "        \"\"\"\n",
    "        Rotate half of the embedding.\n",
    "        \n",
    "        x = [x_0, x_1, x_2, x_3, ...]\n",
    "        output = [-x_1, x_0, -x_3, x_2, ...]\n",
    "        \"\"\"\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        return torch.cat([-x2, x1], dim=-1)\n",
    "    \n",
    "    def forward(self, x, start_pos=0):\n",
    "        \"\"\"\n",
    "        Apply rotary embedding to input.\n",
    "        \n",
    "        Args:\n",
    "            x: [B, T, D] or [B, H, T, D] input tensor\n",
    "            start_pos: Starting position offset\n",
    "        \n",
    "        Returns:\n",
    "            Rotated embeddings of same shape\n",
    "        \"\"\"\n",
    "        B, T = x.shape[0], x.shape[1]\n",
    "        \n",
    "        # Compute positions: [T]\n",
    "        positions = torch.arange(start_pos, start_pos + T, device=x.device).float()\n",
    "        \n",
    "        # Compute phase for each position and dimension: [T, D/2]\n",
    "        # phase = position * frequency\n",
    "        phase = torch.einsum('p,f->pf', positions, self.freqs)\n",
    "        \n",
    "        # Expand to full embedding dimension: [T, D]\n",
    "        # Interleave cos and sin\n",
    "        cos = torch.cos(phase)\n",
    "        sin = torch.sin(phase)\n",
    "        \n",
    "        # Build full cos/sin for this shape\n",
    "        cos = self._interleave(cos, x.shape[-1])\n",
    "        sin = self._interleave(sin, x.shape[-1])\n",
    "        \n",
    "        # Apply rotation using the formula:\n",
    "        # RoPE(x_pos) = x * cos(theta) + rotate_half(x) * sin(theta)\n",
    "        return x * cos + self.rotate_half(x) * sin\n",
    "    \n",
    "    def _interleave(self, half, target_dim):\n",
    "        \"\"\"Interleave half-dimension to match full dimension.\"\"\"\n",
    "        result = torch.zeros(*half.shape[:-1], target_dim, device=half.device)\n",
    "        result[..., ::2] = half\n",
    "        result[..., 1::2] = half\n",
    "        return result\n",
    "\n",
    "# Test RotaryEmbedding\n",
    "print(\"Testing RotaryEmbedding...\")\n",
    "rope = RotaryEmbedding(EMB_SIZE)\n",
    "\n",
    "x = torch.randn(2, 10, EMB_SIZE)\n",
    "y = rope(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "# Verify relative position encoding\n",
    "print(\"\\n--- Verifying Relative Position Encoding ---\")\n",
    "x_test = torch.zeros(1, 3, EMB_SIZE)  # Simple test\n",
    "x_test[0, 1, 0] = 1.0  # Only activate position 1\n",
    "\n",
    "y_at_pos0 = rope(x_test.clone(), start_pos=0)\n",
    "y_at_pos5 = rope(x_test.clone(), start_pos=5)\n",
    "\n",
    "print(\"Input: Only position 1 is activated\")\n",
    "print(f\"At position 0: {y_at_pos0[0, 0, :4].tolist()}\")\n",
    "print(f\"At position 5: {y_at_pos5[0, 5, :4].tolist()}\")\n",
    "print(\"Note: Different positions produce different embeddings!\")\n",
    "\n",
    "assert y.shape == x.shape\n",
    "print(\"\\n✓ RotaryEmbedding working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention with GQA\n",
    "\n",
    "**Grouped Query Attention (GQA)**:\n",
    "- Multiple **query** heads (Q)\n",
    "- Fewer **key/value** heads (K, V)\n",
    "- Each K/V head is shared by multiple Q heads\n",
    "\n",
    "**Benefits**:\n",
    "- Memory efficient (fewer K/V projections and attention maps)\n",
    "- Quality similar to full attention\n",
    "\n",
    "**Example**:\n",
    "- Q heads: 8, KV heads: 2\n",
    "- Each KV head is shared by 4 Q heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention with Grouped Query Attention (GQA).\n",
    "    \n",
    "    Features:\n",
    "    - Grouped Query Attention: Fewer KV heads than Q heads\n",
    "    - Rotary Embedding (RoPE) for position encoding\n",
    "    - Soft attention capping: tanh(x/c) * c\n",
    "    \n",
    "    Args:\n",
    "        dim: Model dimension\n",
    "        num_q_heads: Number of query heads\n",
    "        num_kv_heads: Number of key/value heads (usually < num_q_heads)\n",
    "        key_size: Dimension per head (default: dim // num_q_heads)\n",
    "        cap: Attention logits cap (soft capping)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, num_q_heads, num_kv_heads, key_size=None, cap=30.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
        "self.num_q_heads = num_q_heads\n",
        "self.num_kv_heads = num_kv_heads\n",
        "self.key_size = key_size or (dim // num_q_heads)\n",
        "self.cap = cap\n",
        "\n",
        "        # Compute value size\n",
        "        self.value_size = self.key_size\n",
        "\n",
        "        # Q projection: dim -> num_q_heads * key_size\n",
        "        self.q_proj = nn.Linear(dim, num_q_heads * self.key_size, bias=False)\n",
        "        \n",
        "        # KV projection: dim -> num_kv_heads * key_size (and value_size)\n",
        "        self.k_proj = nn.Linear(dim, num_kv_heads * self.key_size, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, num_kv_heads * self.value_size, bias=False)\n",
        "        \n",
        "        # Output projection: num_q_heads * value_size -> dim\n",
        "        self.o_proj = nn.Linear(num_q_heads * self.value_size, dim, bias=False)\n",
        "        \n",
        "        # RoPE\n",
        "        self.rope = RotaryEmbedding(self.key_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, D] input embeddings\n",
    "            attention_mask: [B, T] or [B, 1, T, T] mask (True=attend)\n",
    "            key_padding_mask: [B, T] mask for padding (True=mask)\n",
    "        \n",
        "        Returns:\n",
    "            Output embeddings [B, T, D]\n",
        "        \"\"\"\n",
    "        B, T, D = x.shape\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        q = self.q_proj(x).view(B, T, self.num_q_heads, self.key_size)\n",
        "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.key_size)\n",
        "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.value_size)\n",
        "\n",
        "        # Apply RoPE to Q and K\n",
        "        q = self.rope(q)\n",
        "        k = self.rope(k)\n",
        "\n",
        "        # GQA: Repeat KV heads to match Q heads\n",
        "        # [B, T, num_kv_heads, D] -> [B, T, num_q_heads, D]\n",
        "        if self.num_kv_heads < self.num_q_heads:\n",
    "            repeats = self.num_q_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeats, dim=2)\n",
    "            v = v.repeat_interleave(repeats, dim=2)\n",
    "\n",
    "        # Transpose for attention: [B, H, T, D]\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
        "        # Compute attention scores\n",
        "        # Q @ K^T / sqrt(key_size)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.key_size ** 0.5)\n",
    "\n",
        "        # Soft attention capping: tanh(x/c) * c\n",
        "        # Prevents extreme attention weights\n",
        "        scores = self.cap * torch.tanh(scores / self.cap)\n",
        "\n",
        "        # Apply masks\n",
        "        if key_padding_mask is not None:\n",
    "            # Convert padding mask to attention mask\n",
    "            scores = scores.masked_fill(~key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(~attention_mask, float('-inf'))\n",
        "\n",
    "        # Softmax\n",
    "        attn_weights = F.softmax(scores.float(), dim=-1)\n",
    "\n",
        "        # Weighted sum\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Concatenate heads and project\n",
    "        output = output.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Test MultiHeadAttention\n",
    "print(\"Testing MultiHeadAttention with GQA...\")\n",
    "mha = MultiHeadAttention(\n",
    "    dim=EMB_SIZE,\n",
    "    num_q_heads=NUM_Q_HEADS,\n",
    "    num_kv_heads=NUM_KV_HEADS,\n",
    "    key_size=KEY_SIZE\n",
    ")\n",
    "\n",
    "B, T = 2, 8\n",
    "x = torch.randn(B, T, EMB_SIZE)\n",
    "padding_mask = torch.ones(B, T, dtype=torch.bool)\n",
    "padding_mask[:, 6:] = False  # Last 2 positions are padding\n",
    "\n",
    "y = mha(x, key_padding_mask=~padding_mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"GQA ratio: {NUM_Q_HEADS} Q heads / {NUM_KV_HEADS} KV heads = {NUM_Q_HEADS // NUM_KV_HEADS}x sharing\")\n",
    "\n",
    "assert y.shape == x.shape\n",
    "print(\"✓ MultiHeadAttention working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwiGLU Feed-Forward Network\n",
    "\n",
    "**SwiGLU** = Swish activation + Gated Linear Unit.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "FFN(x) = (SiLU(x @ W_1) * x @ W_2) @ W_3\n",
    "```\n",
    "\n",
    "**Components**:\n",
    "- **SiLU** (Swish): x * sigmoid(x)\n",
    "- **Gate**: Element-wise multiplication creates a \"veto\" mechanism\n",
    "- **Widening factor**: Hidden dimension is 4/3 of input (rounded to multiple of 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn_size(emb_size, widening_factor=4.0):\n",
    "    \"\"\"\n",
    "    Compute feed-forward hidden size with widening factor.\n",
    "    \n",
    "    Grok uses: hidden = int(widening_factor * emb_size) * 2 // 3\n",
    "    Then rounds to multiple of 8.\n",
    "    \"\"\"\n",
    "    hidden = int(widening_factor * emb_size) * 2 // 3\n",
    "    hidden = hidden + (8 - hidden) % 8  # Round to multiple of 8\n",
    "    return hidden\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU Feed-Forward Network (DenseBlock).\n",
    "    \n",
    "    FFN(x) = (SiLU(x @ W_1) * x @ W_2) @ W_3\n",
    "    \n",
    "    Args:\n",
    "        dim: Input dimension\n",
    "        hidden_dim: Hidden dimension (default: computed from widening factor)\n",
    "        widening_factor: How much to widen (default: 4.0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, hidden_dim=None, widening_factor=4.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = ffn_size(dim, widening_factor)\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # W_1: dim -> hidden\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # W_2: dim -> hidden (gate projection)\n",
    "        self.w2 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # W_3: hidden -> dim (output projection)\n",
    "        self.w3 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, D] input\n",
    "        \n",
    "        Returns:\n",
    "            Output [B, T, D]\n",
    "        \"\"\"\n",
    "        # SiLU(x @ W_1)\n",
    "        gate = F.silu(self.w1(x))\n",
    "        \n",
    "        # x @ W_2\n",
    "        up = self.w2(x)\n",
    "        \n",
    "        # Element-wise multiply (gate)\n",
    "        gated = gate * up\n",
    "        \n",
    "        # @ W_3\n",
    "        return self.w3(gated)\n",
    "\n",
    "# Test SwiGLUFFN\n",
    "print(\"Testing SwiGLUFFN...\")\n",
    "\n",
    "ffn = SwiGLUFFN(EMB_SIZE)\n",
    "\n",
    "print(f\"Input dim: {EMB_SIZE}\")\n",
    "print(f\"Hidden dim: {ffn.hidden_dim}\")\n",
    "print(f\"Widening factor: {(ffn.hidden_dim * 3) / (2 * EMB_SIZE):.1f}x\")\n",
    "\n",
    "x = torch.randn(2, 8, EMB_SIZE)\n",
    "y = ffn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "assert y.shape == x.shape\n",
    "print(\"✓ SwiGLUFFN working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecoderLayer\n",
    "\n",
    "Complete transformer decoder layer with:\n",
    "1. RMSNorm\n",
    "2. Multi-Head Attention with residual\n",
    "3. RMSNorm\n",
    "4. SwiGLU FFN with residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder Layer.\n",
    "    \n",
    "    Architecture:\n",
    "    1. x' = RMSNorm(x)\n",
    "    2. h = MHA(x') + x (residual)\n",
    "    3. x'' = RMSNorm(h)\n",
    "    4. y = SwiGLU(x'') + x'' (residual from layer start)\n",
    "    \n",
    "    Note: Second residual is from h, not x (like some implementations).\n",
    "    \n",
    "    Args:\n",
    "        dim: Model dimension\n",
    "        num_q_heads: Number of query heads\n",
    "        num_kv_heads: Number of key/value heads\n",
    "        key_size: Dimension per head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, num_q_heads, num_kv_heads, key_size=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pre-attention norm\n",
    "        self.attn_norm = RMSNorm(dim)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attn = MultiHeadAttention(\n",
    "            dim=dim,\n",
    "            num_q_heads=num_q_heads,\n",
    "            num_kv_heads=num_kv_heads,\n",
    "            key_size=key_size\n",
    "        )\n",
    "        \n",
    "        # Pre-FFN norm\n",
    "        self.ffn_norm = RMSNorm(dim)\n",
    "        \n",
    "        # SwiGLU FFN\n",
    "        self.ffn = SwiGLUFFN(dim)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, D] input\n",
    "            attention_mask: [B, T] or [B, 1, T, T] mask\n",
    "            key_padding_mask: [B, T] padding mask\n",
    "        \n",
    "        Returns:\n",
    "            Output [B, T, D]\n",
    "        \"\"\"\n",
    "        # Attention block\n",
    "        h = self.attn_norm(x)\n",
    "        h = self.attn(h, key_padding_mask=key_padding_mask)\n",
    "        h = h + x  # Residual from input\n",
    "        \n",
    "        # FFN block\n",
    "        h = self.ffn_norm(h)\n",
    "        h = self.ffn(h)\n",
    "        h = h + x  # Residual from start of layer\n",
    "        \n",
    "        return h\n",
    "\n",
    "# Test DecoderLayer\n",
    "print(\"Testing DecoderLayer...\")\n",
    "layer = DecoderLayer(\n",
    "    dim=EMB_SIZE,\n",
    "    num_q_heads=NUM_Q_HEADS,\n",
    "    num_kv_heads=NUM_KV_HEADS,\n",
    "    key_size=KEY_SIZE\n",
    ")\n",
    "\n",
    "B, T = 2, 8\n",
    "x = torch.randn(B, T, EMB_SIZE)\n",
    "padding_mask = torch.ones(B, T, dtype=torch.bool)\n",
    "padding_mask[:, 6:] = False\n",
    "\n",
    "y = layer(x, key_padding_mask=~padding_mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "assert y.shape == x.shape\n",
    "print(\"✓ DecoderLayer working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_recsys_attn_mask\n",
    "\n",
    "Create attention mask for recommendation system inference.\n",
    "\n",
    "**Key feature**: Candidate isolation!\n",
    "\n",
    "**Mask structure**:\n",
    "- User + History: Can attend to all previous positions (causal)\n",
    "- Candidates: Can attend to User + History AND themselves\n",
    "- **CANNOT** attend to other candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recsys_attn_mask(seq_len, candidate_start_offset, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Create attention mask for recommendation system inference.\n",
    "    \n",
    "    The mask ensures:\n",
    "    1. User + History: Causal attention (can attend to previous positions)\n",
    "    2. Candidates: Can attend to User + History and themselves\n",
    "    3. Candidates: CANNOT attend to other candidates (candidate isolation)\n",
    "    \n",
    "    Sequence layout:\n",
    "    [0] = User\n",
    "    [1:S] = History (S positions)\n",
    "    [S:S+C] = Candidates (C positions)\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Total sequence length\n",
    "        candidate_start_offset: Position where candidates start\n",
    "        dtype: Output dtype\n",
    "    \n",
    "    Returns:\n",
    "        Attention mask [1, 1, seq_len, seq_len] where True=can attend\n",
    "    \"\"\"\n",
    "    # Start with causal mask (lower triangle)\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=dtype))\n",
    "    \n",
    "    # Create the mask\n",
    "    attn_mask = causal_mask.clone()\n",
    "    \n",
    "    # Zero out candidate-to-candidate attention\n",
    "    # This is the bottom-right block\n",
    "    attn_mask[candidate_start_offset:, candidate_start_offset:] = 0\n",
    "    \n",
    "    # Add back self-attention for candidates (diagonal of candidate block)\n",
    "    for i in range(candidate_start_offset, seq_len):\n",
    "        attn_mask[i, i] = 1\n",
    "    \n",
    "    # Add batch and head dimensions\n",
    "    attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    return attn_mask\n",
    "\n",
    "# Test make_recsys_attn_mask\n",
    "print(\"Testing make_recsys_attn_mask...\")\n",
    "\n",
    "USER_LEN = 1\n",
    "HISTORY_LEN = 5\n",
    "CANDIDATE_LEN = 3\n",
    "seq_len = USER_LEN + HISTORY_LEN + CANDIDATE_LEN\n",
    "candidate_start_offset = USER_LEN + HISTORY_LEN\n",
    "\n",
    "mask = make_recsys_attn_mask(seq_len, candidate_start_offset)\n",
    "\n",
    "print(f\"Sequence layout: [User] + [History x{HISTORY_LEN}] + [Candidates x{CANDIDATE_LEN}]\")\n",
    "print(f\"Total seq_len: {seq_len}\")\n",
    "print(f\"Candidate start offset: {candidate_start_offset}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "mask_np = mask.squeeze().numpy()\n",
    "im = ax.imshow(mask_np, cmap='Blues', interpolation='nearest')\n",
    "ax.axvline(x=candidate_start_offset - 0.5, color='red', linestyle='--', linewidth=2)\n",
    "ax.axhline(y=candidate_start_offset - 0.5, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Position (can attend to)')\n",
    "ax.set_ylabel('Position (query)')\n",
    "ax.set_title('Attention Mask\\n(White=Can Attend, Blue=Cannot)')\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(['U'] + [f'H{i}' for i in range(HISTORY_LEN)] + [f'C{i}' for i in range(CANDIDATE_LEN)])\n",
    "ax.set_yticklabels(['U'] + [f'H{i}' for i in range(HISTORY_LEN)] + [f'C{i}' for i in range(CANDIDATE_LEN)])\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMask regions:\")\n",
    "print(\"  UL: User + History -> User + History (causal)\")\n",
    "print(\"  UR: User + History -> Candidates (allowed)\")\n",
    "print(\"  LL: Candidates -> User + History (allowed)\")\n",
    "print(\"  LR diagonal: Candidates -> Themselves (allowed)\")\n",
    "print(\"  LR off-diagonal: Candidates -> Other candidates (BLOCKED)\")\n",
    "print(\"\\n✓ Candidate isolation verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Complete transformer stack with optional candidate isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder for recommendation systems.\n",
    "    \n",
    "    Features:\n",
    "    - Stacks of DecoderLayers\n",
    "    - Optional candidate isolation via custom attention mask\n",
    "    - RMSNorm throughout\n",
    "    \n",
    "    Args:\n",
    "        dim: Model dimension\n",
    "        num_layers: Number of decoder layers\n",
    "        num_q_heads: Number of query heads per layer\n",
    "        num_kv_heads: Number of key/value heads per layer\n",
    "        key_size: Dimension per head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, num_layers, num_q_heads, num_kv_heads, key_size=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Stack of decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(dim, num_q_heads, num_kv_heads, key_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final norm\n",
    "        self.norm = RMSNorm(dim)\n",
    "    \n",
    "    def forward(self, x, padding_mask=None, candidate_start_offset=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, T, D] input embeddings\n",
    "            padding_mask: [B, T] True for valid positions\n",
    "            candidate_start_offset: Position where candidates start (None for standard causal)\n",
    "        \n",
    "        Returns:\n",
    "            Output embeddings [B, T, D]\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        # Create attention mask\n",
    "        if candidate_start_offset is not None:\n",
    "            # Use candidate isolation mask\n",
    "            attn_mask = make_recsys_attn_mask(T, candidate_start_offset, x.dtype)\n",
    "            attn_mask = attn_mask.to(x.device)\n",
    "            \n",
    "            # Combine with padding mask\n",
    "            if padding_mask is not None:\n",
    "                # padding_mask: [B, T] -> [B, 1, 1, T]\n",
    "                padding = (~padding_mask).unsqueeze(1).unsqueeze(1)\n",
    "                attn_mask = attn_mask & ~padding  # True = can attend\n",
    "        else:\n",
    "            # Standard causal mask\n",
    "            causal = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "            attn_mask = causal.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            if padding_mask is not None:\n",
    "                padding = (~padding_mask).unsqueeze(1).unsqueeze(1)\n",
    "                attn_mask = attn_mask & ~padding\n",
    "        \n",
    "        # Pass through all layers\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, attention_mask=attn_mask)\n",
    "        \n",
    "        # Final norm\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "# Test Transformer\n",
    "print(\"Testing Transformer...\")\n",
    "\n",
    "transformer = Transformer(\n",
    "    dim=EMB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_q_heads=NUM_Q_HEADS,\n",
    "    num_kv_heads=NUM_KV_HEADS,\n",
    "    key_size=KEY_SIZE\n",
    ")\n",
    "\n",
    "B, T = 2, 10\n",
    "x = torch.randn(B, T, EMB_SIZE)\n",
    "padding_mask = torch.ones(B, T, dtype=torch.bool)\n",
    "padding_mask[:, 8:] = False\n",
    "\n",
    "# Test with candidate isolation\n",
    "candidate_start = 5\n",
    "y = transformer(x, padding_mask=padding_mask, candidate_start_offset=candidate_start)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"With candidate isolation starting at position: {candidate_start}\")\n",
    "\n",
    "assert y.shape == x.shape\n",
    "print(\"✓ Transformer working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: With vs Without Candidate Isolation\n",
    "\n",
    "Compare transformer outputs with and without candidate isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DEMO: With vs Without Candidate Isolation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "transformer = Transformer(\n",
    "    dim=EMB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_q_heads=NUM_Q_HEADS,\n",
    "    num_kv_heads=NUM_KV_HEADS,\n",
    "    key_size=KEY_SIZE\n",
    ")\n",
    "\n",
    "B = 2\n",
    "USER_LEN = 1\n",
    "HISTORY_LEN = 5\n",
    "CANDIDATE_LEN = 3\n",
    "T = USER_LEN + HISTORY_LEN + CANDIDATE_LEN\n",
    "candidate_start = USER_LEN + HISTORY_LEN\n",
    "\n",
    "x = torch.randn(B, T, EMB_SIZE, requires_grad=True)\n",
    "padding_mask = torch.ones(B, T, dtype=torch.bool)\n",
    "\n",
    "# Forward pass WITHOUT candidate isolation\n",
    "y_no_iso = transformer(x, padding_mask=padding_mask, candidate_start_offset=None)\n",
    "\n",
    "# Forward pass WITH candidate isolation\n",
    "y_iso = transformer(x, padding_mask=padding_mask, candidate_start_offset=candidate_start)\n",
    "\n",
    "print(f\"Input: {B} users, {T} positions\")\n",
    "print(f\"Positions: [1 user] + [{HISTORY_LEN} history] + [{CANDIDATE_LEN} candidates]\")\n",
    "print(f\"\\nOutput shapes (should be same):\")\n",
    "print(f\"  Without isolation: {y_no_iso.shape}\")\n",
    "print(f\"  With isolation: {y_iso.shape}\")\n",
    "\n",
    "# Check if outputs differ\n",
    "diff = torch.abs(y_no_iso - y_iso)\n",
    "print(f\"\\nMax difference between outputs: {diff.max().item():.6f}\")\n",
    "print(f\"Mean difference between outputs: {diff.mean().item():.6f}\")\n",
    "\n",
    "# Verify candidate isolation property\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Verifying Candidate Isolation Property\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Modify only candidate 2\n",
    "x_modified = x.clone()\n",
    "x_modified[:, candidate_start + 1, :] += 100.0  # Big change to candidate 1\n",
    "\n",
    "# Get outputs with isolation\n",
    "y_iso_orig = transformer(x, padding_mask=padding_mask, candidate_start_offset=candidate_start)\n",
    "y_iso_mod = transformer(x_modified, padding_mask=padding_mask, candidate_start_offset=candidate_start)\n",
    "\n",
    "# Check effect on each candidate\n",
    "print(\"\\nAfter modifying candidate 1 (index 6):\")\n",
    "for i in range(CANDIDATE_LEN):\n",
    "    cand_idx = candidate_start + i\n",
    "    diff = torch.abs(y_iso_orig[0, cand_idx] - y_iso_mod[0, cand_idx]).max().item()\n",
    "    if i == 1:  # Modified candidate\n",
    "        print(f\"  Candidate {i} (modified): output changed by {diff:.4f} ✓\")\n",
    "    else:  # Other candidates\n",
    "        print(f\"  Candidate {i} (unchanged): output changed by {diff:.8f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Insight:\")\n",
    "print(\"Without candidate isolation, all positions attend to each other.\")\n",
    "print(\"With candidate isolation, candidates cannot see other candidates.\")\n",
    "print(\"This ensures fair scoring and no information leakage.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Attention Patterns\n",
    "\n",
    "Show how attention differs with and without candidate isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple attention model to visualize patterns\n",
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"Simplified transformer for visualization.\"\"\"\n",
    "    def __init__(self, dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(dim, dim) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        return x\n",
    "\n",
    "# Use the attention mask directly\n",
    "USER_LEN = 1\n",
    "HISTORY_LEN = 4\n",
    "CANDIDATE_LEN = 3\n",
    "T = USER_LEN + HISTORY_LEN + CANDIDATE_LEN\n",
    "candidate_start = USER_LEN + HISTORY_LEN\n",
    "\n",
    "# Create masks\n",
    "causal_mask = torch.tril(torch.ones(T, T))\n",
    "recsys_mask = make_recsys_attn_mask(T, candidate_start)\n",
    "\n",
    "# Convert to numpy for visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Standard causal mask\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(causal_mask.numpy(), cmap='Blues')\n",
    "ax1.axvline(x=candidate_start - 0.5, color='red', linestyle='--', linewidth=2)\n",
    "ax1.axhline(y=candidate_start - 0.5, color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_title('Standard Causal Mask\\n(Candidates can see each other)')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Position')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Recsys attention mask\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(recsys_mask.squeeze().numpy(), cmap='Blues')\n",
    "ax2.axvline(x=candidate_start - 0.5, color='red', linestyle='--', linewidth=2)\n",
    "ax2.axhline(y=candidate_start - 0.5, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_title('RecSys Attention Mask\\n(Candidates CANNOT see each other)')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Position')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "# Add labels\n",
    "labels = ['U'] + [f'H{i}' for i in range(HISTORY_LEN)] + [f'C{i}' for i in range(CANDIDATE_LEN)]\n",
    "for ax in axes:\n",
    "    ax.set_xticks(range(T))\n",
    "    ax.set_yticks(range(T))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key differences:\")\n",
    "print(\"1. Standard: Lower-right block has full attention (candidates see each other)\")\n",
    "print(\"2. RecSys: Lower-right block only has diagonal (candidates see only themselves)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you implemented the Grok transformer building blocks:\n",
    "\n",
    "1. **RMSNorm**: Root Mean Square normalization (simpler than LayerNorm)\n",
    "2. **RotaryEmbedding**: RoPE for relative position encoding\n",
    "3. **MultiHeadAttention**: GQA attention with RoPE and soft capping\n",
    "4. **SwiGLUFFN**: Swish-gated feed-forward network\n",
    "5. **DecoderLayer**: Complete transformer layer with residuals\n",
    "6. **make_recsys_attn_mask**: Candidate isolation attention mask\n",
    "7. **Transformer**: Complete transformer stack\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **GQA**: Memory-efficient attention (fewer KV heads)\n",
    "- **RoPE**: Better relative position encoding (no learned positions)\n",
    "- **Soft capping**: Prevents extreme attention weights\n",
    "- **SwiGLU**: Better than ReLU for transformers\n",
    "- **Candidate isolation**: Critical for fair ranking\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Lecture 6**: Scoring Pipeline - see how the transformer fits into the full system\n",
    "- **Lecture 7**: Replication Guide - build your own recommender!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
