{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Ranking Model - Interactive Notebook\n",
    "\n",
    "> **Companion to**: 04-ranking-model.md\n",
    "> **Run time**: ~5 minutes\n",
    "\n",
    "This notebook implements the Phoenix ranking transformer from X's recommendation algorithm.\n",
    "\n",
    "**Key concepts covered:**\n",
    "- PhoenixModel architecture\n",
    "- [user, history, candidates] sequence\n",
    "- candidate_start_offset and candidate isolation\n",
    "- Unembedding -> sigmoid -> 19 probabilities\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "EMB_SIZE = 64\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_ACTIONS = 19\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 4\n",
    "KEY_SIZE = 16\n",
    "HISTORY_LEN = 10\n",
    "NUM_CANDIDATES = 5\n",
    "\n",
    "# Action names (from runners.py:202-222)\n",
    "ACTION_NAMES = [\n",
    "    'favorite_score', 'reply_score', 'repost_score', 'photo_expand_score',\n",
    "    'click_score', 'profile_click_score', 'vqv_score', 'share_score',\n",
    "    'share_via_dm_score', 'share_via_copy_link_score', 'dwell_score',\n",
    "    'quote_score', 'quoted_click_score', 'follow_author_score',\n",
    "    'not_interested_score', 'block_author_score', 'mute_author_score',\n",
    "    'report_score', 'dwell_time'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reuse Components from Previous Notebooks\n",
    "\n",
    "Copy the essential building blocks from the transformer notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(torch.mean(x.float() ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms * self.scale).to(x.dtype)\n",
    "\n",
    "# RotaryEmbedding\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        freqs = torch.arange(0, dim, 2, dtype=torch.float32) / dim\n",
    "        self.register_buffer('freqs', 1.0 / (base ** freqs))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> torch.Tensor:\n",
    "        positions = torch.arange(seq_len, dtype=torch.float32, device=x.device)\n",
    "        angles = positions.unsqueeze(-1) * self.freqs.to(x.device)\n",
    "        cos = torch.cos(angles)\n",
    "        sin = torch.sin(angles)\n",
    "        x_half = torch.reshape(x.float(), (*x.shape[:-1], -1, 2))\n",
    "        x0, x1 = x_half[..., 0], x_half[..., 1]\n",
    "        rotated_x0 = x0 * cos - x1 * sin\n",
    "        rotated_x1 = x0 * sin + x1 * cos\n",
    "        rotated = torch.stack([rotated_x0, rotated_x1], dim=-1)\n",
    "        return torch.reshape(rotated, x.shape).to(x.dtype)\n",
    "\n",
    "# MultiHeadAttention with GQA, RoPE, Capping\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_q_heads: int, num_kv_heads: int, key_size: int):\n",
    "        super().__init__()\n",
    "        self.num_q_heads = num_q_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.key_size = key_size\n",
    "        self.num_groups = num_q_heads // num_kv_heads\n",
    "        self.wq = nn.Linear(d_model, num_q_heads * key_size, bias=False)\n",
    "        self.wk = nn.Linear(d_model, num_kv_heads * key_size, bias=False)\n",
    "        self.wv = nn.Linear(d_model, num_kv_heads * key_size, bias=False)\n",
    "        self.wo = nn.Linear(num_q_heads * key_size, d_model, bias=False)\n",
    "        self.rope = RotaryEmbedding(key_size)\n",
    "        self.max_attn_val = 30.0\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        B, T, D = query.shape\n",
    "        q = self.wq(query).view(B, T, self.num_q_heads, self.key_size)\n",
    "        k = self.wk(key).view(B, T, self.num_kv_heads, self.key_size)\n",
    "        v = self.wv(value).view(B, T, self.num_kv_heads, self.key_size)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "        q = self.rope(q, T)\n",
    "        k = self.rope(k, T)\n",
    "        k = k.repeat_interleave(self.num_groups, dim=1)\n",
    "        v = v.repeat_interleave(self.num_groups, dim=1)\n",
    "        attn_logits = torch.einsum('bhtk,bhkt->bhtt', q, k) / (self.key_size ** 0.5)\n",
    "        attn_logits = self.max_attn_val * torch.tanh(attn_logits / self.max_attn_val)\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(~mask.bool(), float('-inf'))\n",
    "        attn_weights = F.softmax(attn_logits.float(), dim=-1)\n",
    "        attn_out = torch.einsum('bhtt,bhtk->bhtk', attn_weights, v)\n",
    "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        return self.wo(attn_out)\n",
    "\n",
    "# SwiGLUFFN\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, d_model: int, widening_factor: float = 4.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(widening_factor * d_model) // 3 * 4\n",
    "        hidden_dim = (hidden_dim + 7) // 8 * 8\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(F.silu(self.w1(x)) * self.v(x))\n",
    "\n",
    "# DecoderLayer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_q_heads: int, num_kv_heads: int, key_size: int):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_q_heads, num_kv_heads, key_size)\n",
    "        self.ffn = SwiGLUFFN(d_model)\n",
    "        self.attn_norm = RMSNorm(d_model)\n",
    "        self.ffn_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        x = self.attn_norm(x + self.mha(x, x, x, mask))\n",
    "        x = self.ffn_norm(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "# Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_q_heads: int, num_kv_heads: int, key_size: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_q_heads, num_kv_heads, key_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, padding_mask: torch.Tensor = None,\n",
    "                candidate_start_offset: int = None) -> torch.Tensor:\n",
    "        B, T, D = embeddings.shape\n",
    "        if padding_mask is not None:\n",
    "            attn_mask = padding_mask.unsqueeze(1).unsqueeze(2) & padding_mask.unsqueeze(1).unsqueeze(2).transpose(-2, -1)\n",
    "        else:\n",
    "            attn_mask = torch.ones(B, 1, 1, T, dtype=torch.bool, device=embeddings.device)\n",
    "        if candidate_start_offset is not None:\n",
    "            causal = torch.tril(torch.ones(1, 1, T, T, dtype=torch.bool, device=embeddings.device))\n",
    "            recsys_mask = torch.ones(1, 1, T, T, dtype=torch.bool, device=embeddings.device)\n",
    "            recsys_mask[:, :, candidate_start_offset:, candidate_start_offset:] = False\n",
    "            for i in range(candidate_start_offset, T):\n",
    "                recsys_mask[:, :, i, i] = True\n",
    "            attn_mask = attn_mask & causal & recsys_mask\n",
    "        x = embeddings\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask)\n",
    "        return self.final_norm(x)\n",
    "\n",
    "print(\"Components from previous notebooks loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BlockReduce Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockUserReduce(nn.Module):\n",
    "    \"\"\"Combine user hash embeddings into single representation.\"\"\"\n",
    "    def __init__(self, num_hashes: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(num_hashes * emb_size, emb_size, bias=False)\n",
    "\n",
    "    def forward(self, user_embeddings: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, _, D = user_embeddings.shape\n",
",
    "        user_flat = user_embeddings.reshape(B, -1)\n",
    "        user_proj = self.projection(user_flat)\n",
    "        user_embedding = user_proj.unsqueeze(1)  # [B, 1, D]\n",
    "        user_padding_mask = torch.ones(B, 1, dtype=torch.bool, device=user_embeddings.device)\n",
    "        return user_embedding, user_padding_mask\n",
    "\n",
    "class BlockHistoryReduce(nn.Module):\n",
    "    \"\"\"Combine history ingredients into sequence.\"\"\"\n",
    "    def __init__(self, num_item_hashes: int, num_author_hashes: int, emb_size: int, num_actions: int):\n",
    "        super().__init__()\n",
    "        input_dim = num_item_hashes * emb_size + num_author_hashes * emb_size + emb_size + emb_size\n",
    "        self.projection = nn.Linear(input_dim, emb_size, bias=False)\n",
    "        self.action_proj = nn.Linear(num_actions, emb_size, bias=False)\n",
    "        self.surface_emb = nn.Embedding(16, emb_size)\n",
    "\n",
    "    def forward(self, post_emb, author_emb, actions, surface) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, S, _, D = post_emb.shape\n",
    "        post_flat = post_emb.reshape(B, S, -1)\n",
    "        author_flat = author_emb.reshape(B, S, -1)\n",
    "        actions_signed = (2 * actions - 1).float()\n",
    "        action_emb = self.action_proj(actions_signed)\n",
    "        surface_emb = self.surface_emb(surface)\n",
    "        combined = torch.cat([post_flat, author_flat, action_emb, surface_emb], dim=-1)\n",
    "        history_emb = self.projection(combined)\n",
    "        history_mask = torch.ones(B, S, dtype=torch.bool, device=post_emb.device)\n",
    "        return history_emb, history_mask\n",
    "\n",
    "class BlockCandidateReduce(nn.Module):\n",
    "    \"\"\"Combine candidate ingredients into sequence.\"\"\"\n",
    "    def __init__(self, num_item_hashes: int, num_author_hashes: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        input_dim = num_item_hashes * emb_size + num_author_hashes * emb_size + emb_size\n",
    "        self.projection = nn.Linear(input_dim, emb_size, bias=False)\n",
    "        self.surface_emb = nn.Embedding(16, emb_size)\n",
    "\n",
    "    def forward(self, post_emb, author_emb, surface) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, C, _, D = post_emb.shape\n",
    "        post_flat = post_emb.reshape(B, C, -1)\n",
    "        author_flat = author_emb.reshape(B, C, -1)\n",
    "        surface_emb = self.surface_emb(surface)\n",
    "        combined = torch.cat([post_flat, author_flat, surface_emb], dim=-1)\n",
    "        cand_emb = self.projection(combined)\n",
    "        cand_mask = torch.ones(B, C, dtype=torch.bool, device=post_emb.device)\n",
    "        return cand_emb, cand_mask\n",
    "\n",
    "print(\"BlockReduce modules defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RankingModel Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Phoenix Ranking Model for candidate scoring.\n",
    "\n",
    "    Architecture:\n",
    "    1. Build inputs: [user] + [history] + [candidates]\n",
    "    2. Run transformer with candidate isolation\n",
    "    3. Slice out candidate outputs\n",
    "    4. Apply unembedding -> sigmoid -> 19 probabilities\n",
    "\n",
    "    From recsys_model.py:284-474\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 num_actions: int,\n",
    "                 num_layers: int = NUM_LAYERS,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 key_size: int = KEY_SIZE):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # Block reduction modules\n",
    "        self.block_user = BlockUserReduce(2, emb_size)\n",
    "        self.block_history = BlockHistoryReduce(2, 2, emb_size, num_actions)\n",
    "        self.block_candidate = BlockCandidateReduce(2, 2, emb_size)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_size,\n",
    "            num_q_heads=num_heads,\n",
    "            num_kv_heads=num_heads // 2,  # GQA\n",
    "            key_size=key_size,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Unembedding: project from emb_size to num_actions\n",
    "        self.unembedding = nn.Linear(emb_size, num_actions, bias=False)\n",
    "\n",
    "    def build_inputs(self, user_emb, history_emb, history_mask, cand_emb, cand_mask):\n",
    "        \"\"\"Build input sequence: [user] + [history] + [candidates]\"\"\"\n",
    "        # Concatenate all\n",
    "        embeddings = torch.cat([user_emb, history_emb, cand_emb], dim=1)  # [B, 1+S+C, D]\n",
    "        padding_mask = torch.cat([torch.ones(user_emb.shape[0], 1, dtype=torch.bool, device=user_emb.device),\n",
    "                                  history_mask, cand_mask], dim=1)\n",
    "        candidate_start_offset = 1 + history_emb.shape[1]\n",
    "        return embeddings, padding_mask, candidate_start_offset\n",
    "\n",
    "    def forward(self, user_emb, history_emb, history_mask, cand_emb, cand_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_emb: [B, 2, D] user hash embeddings\n",
    "            history_emb: [B, S, 2, D] history post+author embeddings\n",
    "            history_mask: [B, S] boolean mask\n",
    "            cand_emb: [B, C, 2, D] candidate post+author embeddings\n",
    "            cand_mask: [B, C] boolean mask\n",
    "\n",
    "        Returns:\n",
    "            logits: [B, C, num_actions] raw logits (before sigmoid)\n",
    "            probs: [B, C, num_actions] probabilities after sigmoid\n",
    "        \"\"\"\n",
    "        # Step 1: Reduce block embeddings\n",
    "        user_repr, user_mask = self.block_user(user_emb)\n",
    "        history_repr, history_mask = self.block_history(\n",
    "            history_emb[:, :, :2],  # post\n",
    "            history_emb[:, :, 2:],  # author\n",
    "            torch.zeros(history_emb.shape[0], history_emb.shape[1], self.num_actions).to(history_emb.device),  # actions\n",
    "            torch.zeros(history_emb.shape[0], history_emb.shape[1], dtype=torch.long).to(history_emb.device)  # surface\n",
    "        )\n",
    "        cand_repr, cand_mask = self.block_candidate(\n",
    "            cand_emb[:, :, :2],  # post\n",
    "            cand_emb[:, :, 2:],  # author\n",
    "            torch.zeros(cand_emb.shape[0], cand_emb.shape[1], dtype=torch.long).to(cand_emb.device)\n",
    "        )\n",
    "\n",
    "        # Step 2: Build inputs\n",
    "        embeddings, padding_mask, candidate_start_offset = self.build_inputs(\n",
    "            user_repr, history_repr, history_mask, cand_repr, cand_mask\n",
    "        )\n",
    "\n",
    "        # Step 3: Run transformer with candidate isolation\n",
    "        outputs = self.transformer(\n",
    "            embeddings,\n",
    "            padding_mask,\n",
    "            candidate_start_offset=candidate_start_offset\n",
    "        )\n",
    "\n",
    "        # Step 4: Slice out candidate outputs\n",
    "        cand_outputs = outputs[:, candidate_start_offset:, :]  # [B, C, D]\n",
    "\n",
    "        # Step 5: Unembedding -> sigmoid\n",
    "        logits = self.unembedding(cand_outputs)  # [B, C, num_actions]\n",
    "        probs = torch.sigmoid(logits)  # [B, C, num_actions]\n",
    "\n",
    "        return logits, probs\n",
    "\n",
    "# Test the RankingModel\n",
    "print(\"Testing RankingModel...\")\n",
    "model = RankingModel(EMB_SIZE, NUM_ACTIONS, NUM_LAYERS, NUM_HEADS, KEY_SIZE).to(device)\n",
    "\n",
    "B = 4\n",
    "S, C = HISTORY_LEN, NUM_CANDIDATES\n",
    "\n",
    "# Create dummy embeddings\n",
    "user_emb = torch.randn(B, 2, EMB_SIZE).to(device)\n",
    "history_emb = torch.randn(B, S, 4, EMB_SIZE).to(device)  # 2 post + 2 author\n",
    "history_mask = torch.ones(B, S, dtype=torch.bool).to(device)\n",
    "history_mask[:, S//2:] = False\n",
    "cand_emb = torch.randn(B, C, 4, EMB_SIZE).to(device)\n",
    "cand_mask = torch.ones(B, C, dtype=torch.bool).to(device)\n",
    "\n",
    "logits, probs = model(user_emb, history_emb, history_mask, cand_emb, cand_mask)\n",
    "\n",
    "print(f\"User emb: {user_emb.shape}\")\n",
    "print(f\"History emb: {history_emb.shape}\")\n",
    "print(f\"Candidate emb: {cand_emb.shape}\")\n",
    "print(f\"Logits shape: {logits.shape}\")  # [B, C, 19]\n",
    "print(f\"Probabilities shape: {probs.shape}\")  # [B, C, 19]\n",
    "print(f\"Prob range: [{probs.min():.4f}, {probs.max():.4f}]\")\n",
    "print(f\"✓ RankingModel works!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. WeightedScorer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedScorer(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine 19 action probabilities into 1 final score using configurable weights.\n",
    "\n",
    "    This allows X to tune feed behavior without retraining the model!\n",
    "\n",
    "    From home-mixer/scorers/weighted_scorer.rs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int = NUM_ACTIONS, init_weights: list = None):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # Default weights (can be tuned without retraining)\n",
    "        if init_weights is None:\n",
    "            self.weights = nn.Parameter(torch.tensor([\n",
    "                1.0,   # favorite\n",
    "                3.0,   # reply\n",
    "                1.5,   # repost\n",
    "                0.8,   # photo_expand\n",
    "                0.5,   # click\n",
    "                2.0,   # profile_click\n",
    "                1.2,   # vqv\n",
    "                1.0,   # share\n",
    "                0.8,   # share_via_dm\n",
    "                0.5,   # share_via_copy_link\n",
    "                0.3,   # dwell\n",
    "                1.5,   # quote\n",
    "                0.7,   # quoted_click\n",
    "                5.0,   # follow_author\n",
    "                -5.0,  # not_interested\n",
    "                -20.0, # block_author\n",
    "                -10.0, # mute_author\n",
    "                -15.0, # report\n",
    "                0.01   # dwell_time\n",
    "            ], dtype=torch.float32))\n",
    "        else:\n",
    "            self.weights = nn.Parameter(torch.tensor(init_weights, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, probs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            probs: [B, C, num_actions] action probabilities\n",
    "        Returns:\n",
    "            scores: [B, C] weighted scores\n",
    "        \"\"\"\n",
    "        # Weighted sum: probs @ weights\n",
    "        scores = torch.einsum('bca,a->bc', probs, self.weights)\n",
    "        return scores\n",
    "\n",
    "# Test WeightedScorer\n",
    "print(\"Testing WeightedScorer...\")\n",
    "scorer = WeightedScorer().to(device)\n",
    "\n",
    "# Using probs from previous test\n",
    "scores = scorer(probs)\n",
    "\n",
    "print(f\"Input probs shape: {probs.shape}\")\n",
    "print(f\"Output scores shape: {scores.shape}\")\n",
    "print(f\"Score range: [{scores.min():.4f}, {scores.max():.4f}]\")\n",
    "\n",
    "# Show weight interpretation\n",
    "print(f\"\\nAction weights:\")\n",
    "for name, weight in zip(ACTION_NAMES[:5], scorer.weights[:5].tolist()):\n",
    "    print(f\"  {name}: {weight}\")\n",
    "print(\"  ...\")\n",
    "for name, weight in zip(ACTION_NAMES[-3:], scorer.weights[-3:].tolist()):\n",
    "    print(f\"  {name}: {weight}\")\n",
    "\n",
    "print(f\"\\n✓ WeightedScorer works!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demo: Verify Candidate Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEMO: Verify Candidate Isolation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create model\n",
    "model = RankingModel(EMB_SIZE, NUM_ACTIONS, NUM_LAYERS, NUM_HEADS, KEY_SIZE).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Create two batches that differ only in candidate 2\n",
    "B, C = 1, NUM_CANDIDATES\n",
    "S = HISTORY_LEN\n",
    "\n",
    "user_emb = torch.randn(B, 2, EMB_SIZE).to(device)\n",
    "history_emb = torch.randn(B, S, 4, EMB_SIZE).to(device)\n",
    "history_mask = torch.ones(B, S, dtype=torch.bool).to(device)\n",
    "\n",
    "# Base candidates\n",
    "cand_emb_base = torch.randn(B, C, 4, EMB_SIZE).to(device)\n",
    "\n",
    "# Modified: change candidate 2\n",
    "cand_emb_modified = cand_emb_base.clone()\n",
    "cand_emb_modified[0, 2] = cand_emb_modified[0, 2] + 10.0  # Big change to candidate 2\n",
    "\n",
    "# Forward passes\n",
    "_, probs_base = model(user_emb, history_emb, history_mask, cand_emb_base, torch.ones(B, C, dtype=torch.bool).to(device))\n",
    "_, probs_modified = model(user_emb, history_emb, history_mask, cand_emb_modified, torch.ones(B, C, dtype=torch.bool).to(device))\n",
    "\n",
    "print(f\"\\nCandidate isolation test:\")\n",
    "print(f\"  Changed candidate 2 embeddings by +10.0\")\n",
    "print(f\"\\n  Effect on other candidates (should be ~0):\")\n",
    "for i in range(C):\n",
    "    diff = (probs_base[0, i] - probs_modified[0, i]).abs().mean().item()\n",
    "    marker = \" <- CHANGED\" if i == 2 else \"\"\n",
    "    print(f\"    Candidate {i}: mean prob diff = {diff:.8f}{marker}\")\n",
    "\n",
    "print(f\"\\n✓ Candidate isolation verified! Changing one candidate doesn't affect others.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demo: Action Probabilities Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEMO: Action Probabilities for Candidates\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "B, C = 2, NUM_CANDIDATES\n",
    "S = HISTORY_LEN\n",
    "\n",
    "# Create fresh data\n",
    "torch.manual_seed(123)\n",
    "user_emb = torch.randn(B, 2, EMB_SIZE).to(device)\n",
    "history_emb = torch.randn(B, S, 4, EMB_SIZE).to(device)\n",
    "history_mask = torch.ones(B, S, dtype=torch.bool).to(device)\n",
    "cand_emb = torch.randn(B, C, 4, EMB_SIZE).to(device)\n",
    "cand_mask = torch.ones(B, C, dtype=torch.bool).to(device)\n",
    "\n",
    "# Forward\n",
    "_, probs = model(user_emb, history_emb, history_mask, cand_emb, cand_mask)\n",
    "\n",
    "# Plot heatmap of action probabilities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for b in range(B):\n",
    "    ax = axes[b]\n",
    "    im = ax.imshow(probs[b].detach().cpu().numpy(), cmap='YlOrRd', aspect='auto')\n",
    "    ax.set_xticks(range(NUM_ACTIONS))\n",
    "    ax.set_xticklabels([n.replace('_score', '') for n in ACTION_NAMES], rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_ylabel('Candidate')\n",
    "    ax.set_xlabel('Action')\n",
    "    ax.set_title(f'Batch {b}: Action Probabilities per Candidate')\n",
    "    ax.set_yticks(range(C))\n",
    "    ax.set_yticklabels([f'Candi {i}' for i in range(C)])\n",
    "    plt.colorbar(im, ax=ax, label='Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top actions for each candidate\n",
    "print(f\"\\nTop 3 actions for each candidate:\")\n",
    "for b in range(B):\n",
    "    print(f\"\\n  Batch {b}:\")\n",
    "    for c in range(C):\n",
    "        top_actions = torch.topk(probs[b, c], 3).indices.tolist()\n",
    "        top_probs = probs[b, c, top_actions].tolist()\n",
    "        print(f\"    Candidate {c}: {[(ACTION_NAMES[i], f'{p:.3f}') for i, p in zip(top_actions, top_probs)]}\")\n",
    "\n",
    "# Apply weighted scorer\n",
    "scorer = WeightedScorer().to(device)\n",
    "scores = scorer(probs)\n",
    "\n",
    "print(f\"\\n\\nWeighted scores (for ranking):\")\n",
    "for b in range(B):\n",
    "    print(f\"  Batch {b}: {scores[b].tolist()}\")\n",
    "    ranking = torch.argsort(scores[b], descending=True).tolist()\n",
    "    print(f\"    Ranking: {ranking}\")\n",
    "\n",
    "print(f\"\\n✓ Demo complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've implemented the Phoenix ranking model from X's recommendation algorithm!\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **PhoenixModel architecture**: Concatenates [user] + [history] + [candidates] as input sequence\n",
    "\n",
    "2. **Candidate isolation**: Uses attention mask so each candidate only attends to user+history and itself\n",
    "\n",
    "3. **Multi-action prediction**: Outputs 19 probabilities (one per engagement type)\n",
    "\n",
    "4. **WeightedScorer**: Combines 19 probabilities into 1 score using tunable weights\n",
    "\n",
    "5. **Candidate independence**: Changing one candidate's embeddings doesn't affect other candidates' scores\n",
    "\n",
    "**Next up**: [Lecture 5 - Grok Transformer Deep Dive](05-grok-transformer.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
