@misc{ashDeepBatchActive2020,
  title = {Deep {{Batch Active Learning}} by {{Diverse}}, {{Uncertain Gradient Lower Bounds}}},
  author = {Ash, Jordan T. and Zhang, Chicheng and Krishnamurthy, Akshay and Langford, John and Agarwal, Alekh},
  year = 2020,
  month = feb,
  number = {arXiv:1906.03671},
  eprint = {1906.03671},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.03671},
  urldate = {2026-01-07},
  abstract = {We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/weixiao09/Zotero/storage/JUZ5PLRR/Ash et al. - 2020 - Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds.pdf}
}

@book{axlerLinearAlgebraDone2015,
  title = {Linear {{Algebra Done Right}}},
  author = {Axler, Sheldon},
  year = 2015,
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-11080-6},
  urldate = {2026-01-09},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-3-319-11079-0 978-3-319-11080-6},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/ELXZW4SM/Axler - 2015 - Linear Algebra Done Right.pdf}
}

@misc{bankAutoencoders2021,
  title = {Autoencoders},
  author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
  year = 2021,
  month = apr,
  number = {arXiv:2003.05991},
  eprint = {2003.05991},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.05991},
  urldate = {2026-01-14},
  abstract = {An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/weixiao09/Zotero/storage/VL65KKHM/Bank et al. - 2021 - Autoencoders.pdf}
}

@article{belkinLaplacianEigenmapsDimensionality2003,
  title = {Laplacian {{Eigenmaps}} for {{Dimensionality Reduction}} and {{Data Representation}}},
  author = {Belkin, Mikhail and Niyogi, Partha},
  year = 2003,
  month = jun,
  journal = {Neural Computation},
  volume = {15},
  number = {6},
  pages = {1373--1396},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976603321780317},
  urldate = {2025-12-24},
  abstract = {One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/LPT4342S/Belkin and Niyogi - 2003 - Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.pdf}
}

@article{belkinManifoldRegularizationGeometric,
  title = {Manifold {{Regularization}}: {{A Geometric Framework}} for {{Learning}} from {{Labeled}} and {{Unlabeled Examples}}},
  author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
  abstract = {We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/5QU5QHBS/Belkin et al. - Manifold Regularization A Geometric Framework for Learning from Labeled and Unlabeled Examples.pdf}
}

@book{bishopDeepLearningFoundations2024,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  year = 2024,
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  urldate = {2026-01-07},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/9YJ5PNVI/Bishop and Bishop - 2024 - Deep Learning Foundations and Concepts.pdf}
}

@book{boydConvexOptimization2023,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = 2023,
  edition = {Version 29},
  publisher = {Cambridge University Press},
  address = {Cambridge New York Melbourne New Delhi Singapore},
  isbn = {978-0-521-83378-3},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/8KY6IDYR/Boyd and Vandenberghe - 2023 - Convex optimization.pdf}
}

@article{carlssonTopologyData2009,
  title = {Topology and Data},
  author = {Carlsson, Gunnar},
  year = 2009,
  month = jan,
  journal = {Bulletin of the American Mathematical Society},
  volume = {46},
  number = {2},
  pages = {255--308},
  issn = {0273-0979},
  doi = {10.1090/S0273-0979-09-01249-X},
  urldate = {2025-12-24},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/SNI9KTWU/Carlsson - 2009 - Topology and data.pdf}
}

@article{cohen-steinerStabilityPersistenceDiagrams2007,
  title = {Stability of {{Persistence Diagrams}}},
  author = {{Cohen-Steiner}, David and Edelsbrunner, Herbert and Harer, John},
  year = 2007,
  month = jan,
  journal = {Discrete \& Computational Geometry},
  volume = {37},
  pages = {103--120},
  issn = {0179-5376, 1432-0444},
  doi = {10.1007/s00454-006-1276-5},
  urldate = {2026-01-31},
  abstract = {The persistence diagram of a real-valued function on a topological space is a multiset of points in the extended plane. We prove that under mild assumptions on the function, the persistence diagram is stable: small changes in the function imply only small changes in the diagram. We apply this result to estimating the homology of sets in a metric space and to comparing and classifying geometric shapes.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/RJZ9D36I/Cohen-Steiner et al. - 2007 - Stability of Persistence Diagrams.pdf}
}

@article{coifmanDiffusionMaps2006,
  title = {Diffusion Maps},
  author = {Coifman, Ronald R. and Lafon, St{\'e}phane},
  year = 2006,
  month = jul,
  journal = {Applied and Computational Harmonic Analysis},
  volume = {21},
  number = {1},
  pages = {5--30},
  issn = {10635203},
  doi = {10.1016/j.acha.2006.04.006},
  urldate = {2025-12-24},
  abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/XHNV49UU/Coifman and Lafon - 2006 - Diffusion maps.pdf}
}

@book{cormenIntroductionAlgorithms2009,
  title = {Introduction to Algorithms},
  editor = {Cormen, Thomas H.},
  year = 2009,
  edition = {3. ed},
  publisher = {MIT Press},
  address = {Cambridge, Mass.},
  isbn = {978-0-262-03384-8 978-0-262-53305-8},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/VWQ4FBIM/Cormen - 2009 - Introduction to algorithms.pdf}
}

@article{coxXv6SimpleUnixlike,
  title = {Xv6: A Simple, {{Unix-like}} Teaching Operating System},
  author = {Cox, Russ and Kaashoek, Frans and Morris, Robert},
  file = {/Users/weixiao09/Zotero/storage/479HGK9W/Cox et al. - xv6 a simple, Unix-like teaching operating system.pdf}
}

@book{dayanTheoreticalNeuroscienceComputational2001,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, Laurence F.},
  year = 2001,
  series = {Computational {{Neuroscience}}},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts London, England},
  abstract = {Intro -- Copyright -- Contents -- Preface -- I Neural Encoding and Decoding -- 1 Neural Encoding I: Firing Rates and Spike Statistics -- 1.1 Introduction -- 1.2 Spike Trains and Firing Rates -- 1.3 What Makes a Neuron Fire? -- 1.4 Spike-Train Statistics -- 1.5 The Neural Code -- 1.6 Chapter Summary -- 1.7 Appendices -- 1.8 Annotated Bibliography -- 2 Neural Encoding II: Reverse Correlation and Visual Receptive Fields -- 2.1 Introduction -- 2.2 Estimating Firing Rates -- 2.3 Introduction to the Early Visual System -- 2.4 Reverse-Correlation Methods: Simple Cells -- 2.5 Static Nonlinearities: Complex Cells -- 2.6 Receptive Fields in the Retina and LGN -- 2.7 Constructing V1 Receptive Fields -- 2.8 Chapter Summary -- 2.9 Appendices -- 2.10 Annotated Bibliography -- 3 Neural Decoding -- 3.1 Encoding and Decoding -- 3.2 Discrimination -- 3.3 Population Decoding -- 3.4 Spike-Train Decoding -- 3.5 Chapter Summary -- 3.6 Appendices -- 3.7 Annotated Bibliography -- 4 Information Theory -- 4.1 Entropy and Mutual Information -- 4.2 Information and Entropy Maximization -- 4.3 Entropy and Information for Spike Trains -- 4.4 Chapter Summary -- 4.5 Appendix -- 4.6 Annotated Bibliography -- II Neurons and Neural Circuits -- 5 Model Neurons I: Neuroelectronics -- 5.1 Introduction -- 5.2 Electrical Properties of Neurons -- 5.3 Single-Compartment Models -- 5.4 Integrate-and-Fire Models -- 5.5 Voltage-Dependent Conductances -- 5.6 The Hodgkin-Huxley Model -- 5.7 Modeling Channels -- 5.8 Synaptic Conductances -- 5.9 Synapses on Integrate-and-Fire Neurons -- 5.10 Chapter Summary -- 5.11 Appendices -- 5.12 Annotated Bibliography -- 6 Model Neurons II: Conductances and Morphology -- 6.1 Levels of Neuron Modeling -- 6.2 Conductance-Based Models -- 6.3 The Cable Equation -- 6.4 Multi-compartment Models -- 6.5 Chapter Summary -- 6.6 Appendices -- 6.7 Annotated Bibliography},
  isbn = {978-0-262-04199-7 978-0-262-31142-7},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/LC5U492I/Dayan and Abbott - 2001 - Theoretical neuroscience computational and mathematical modeling of neural systems.pdf}
}

@book{dayanTheoreticalNeuroscienceComputational2001a,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, Laurence F.},
  year = 2001,
  series = {Computational {{Neuroscience}}},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts London, England},
  abstract = {Intro -- Copyright -- Contents -- Preface -- I Neural Encoding and Decoding -- 1 Neural Encoding I: Firing Rates and Spike Statistics -- 1.1 Introduction -- 1.2 Spike Trains and Firing Rates -- 1.3 What Makes a Neuron Fire? -- 1.4 Spike-Train Statistics -- 1.5 The Neural Code -- 1.6 Chapter Summary -- 1.7 Appendices -- 1.8 Annotated Bibliography -- 2 Neural Encoding II: Reverse Correlation and Visual Receptive Fields -- 2.1 Introduction -- 2.2 Estimating Firing Rates -- 2.3 Introduction to the Early Visual System -- 2.4 Reverse-Correlation Methods: Simple Cells -- 2.5 Static Nonlinearities: Complex Cells -- 2.6 Receptive Fields in the Retina and LGN -- 2.7 Constructing V1 Receptive Fields -- 2.8 Chapter Summary -- 2.9 Appendices -- 2.10 Annotated Bibliography -- 3 Neural Decoding -- 3.1 Encoding and Decoding -- 3.2 Discrimination -- 3.3 Population Decoding -- 3.4 Spike-Train Decoding -- 3.5 Chapter Summary -- 3.6 Appendices -- 3.7 Annotated Bibliography -- 4 Information Theory -- 4.1 Entropy and Mutual Information -- 4.2 Information and Entropy Maximization -- 4.3 Entropy and Information for Spike Trains -- 4.4 Chapter Summary -- 4.5 Appendix -- 4.6 Annotated Bibliography -- II Neurons and Neural Circuits -- 5 Model Neurons I: Neuroelectronics -- 5.1 Introduction -- 5.2 Electrical Properties of Neurons -- 5.3 Single-Compartment Models -- 5.4 Integrate-and-Fire Models -- 5.5 Voltage-Dependent Conductances -- 5.6 The Hodgkin-Huxley Model -- 5.7 Modeling Channels -- 5.8 Synaptic Conductances -- 5.9 Synapses on Integrate-and-Fire Neurons -- 5.10 Chapter Summary -- 5.11 Appendices -- 5.12 Annotated Bibliography -- 6 Model Neurons II: Conductances and Morphology -- 6.1 Levels of Neuron Modeling -- 6.2 Conductance-Based Models -- 6.3 The Cable Equation -- 6.4 Multi-compartment Models -- 6.5 Chapter Summary -- 6.6 Appendices -- 6.7 Annotated Bibliography},
  isbn = {978-0-262-04199-7 978-0-262-31142-7},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/UBE86HDU/Dayan and Abbott - 2001 - Theoretical neuroscience computational and mathematical modeling of neural systems.pdf}
}

@misc{daytonBayesianTopologicalConvolutional2025,
  title = {Bayesian {{Topological Convolutional Neural Nets}}},
  author = {Dayton, Sarah Harkins and Everett, Hayden and Schizas, Ioannis and Boothe, David L. and Maroulas, Vasileios},
  year = 2025,
  month = oct,
  number = {arXiv:2510.11704},
  eprint = {2510.11704},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.11704},
  urldate = {2026-01-07},
  abstract = {Convolutional neural networks (CNNs) have been established as the main workhorse in image data processing; nonetheless, they require large amounts of data to train, often produce overconfident predictions, and frequently lack the ability to quantify the uncertainty of their predictions. To address these concerns, we propose a new Bayesian topological CNN that promotes a novel interplay between topology-aware learning and Bayesian sampling. Specifically, it utilizes information from important manifolds to accelerate training while reducing calibration error by placing prior distributions on network parameters and properly learning appropriate posteriors. One important contribution of our work is the inclusion of a consistency condition in the learning cost, which can effectively modify the prior distributions to improve the performance of our novel network architecture. We evaluate the model on benchmark image classification datasets and demonstrate its superiority over conventional CNNs, Bayesian neural networks (BNNs), and topological CNNs. In particular, we supply evidence that our method provides an advantage in situations where training data is limited or corrupted. Furthermore, we show that the new model allows for better uncertainty quantification than standard BNNs since it can more readily identify examples of out-of-distribution data on which it has not been trained. Our results highlight the potential of our novel hybrid approach for more efficient and robust image classification.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/weixiao09/Zotero/storage/CI9WABS6/Dayton et al. - 2025 - Bayesian Topological Convolutional Neural Nets.pdf}
}

@book{deyComputationalTopologyData2022,
  title = {Computational {{Topology}} for {{Data Analysis}}},
  author = {Dey, Tamal Krishna and Wang, Yusu},
  year = 2022,
  month = feb,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781009099950},
  urldate = {2026-01-31},
  abstract = {Topological data analysis (TDA) has emerged recently as a viable tool for analyzing complex data, and the area has grown substantially both in its methodologies and applicability. Providing a computational and algorithmic foundation for techniques in TDA, this comprehensive, self-contained text introduces students and researchers in mathematics and computer science to the current state of the field. The book features a description of mathematical objects and constructs behind recent advances, the algorithms involved, computational considerations, as well as examples of topological structures or ideas that can be used in applications. It provides a thorough treatment of persistent homology together with various extensions -- like zigzag persistence and multiparameter persistence -- and their applications to different types of data, like point clouds, triangulations, or graph data. Other important topics covered include discrete Morse theory, the Mapper structure, optimal generating cycles, as well as recent advances in embedding TDA within machine learning frameworks.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-1-009-09995-0 978-1-009-09816-8},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/N9CC2UKI/Dey and Wang - 2022 - Computational Topology for Data Analysis.pdf}
}

@book{friedbergLinearAlgebra2014,
  title = {Linear Algebra},
  author = {Friedberg, Stephen H. and Insel, Arnold J. and Spence, Lawrence E.},
  year = 2014,
  edition = {Pearson new international edition. Fourth edition},
  publisher = {Pearson},
  address = {Harlow, Essex},
  isbn = {978-1-292-02650-3},
  langid = {english},
  annotation = {OCLC: 856628767},
  file = {/Users/weixiao09/Zotero/storage/W2PG3IYI/Friedberg et al. - 2014 - Linear algebra.pdf}
}

@article{gelmanBayesianDataAnalysis,
  title = {Bayesian {{Data Analysis Third}} Edition (with Errors Fixed as of 20 {{February}} 2025)},
  author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/YTN6EIB3/Gelman et al. - Bayesian Data Analysis Third edition (with errors ﬁxed as of 20 February 2025).pdf}
}

@book{georgeCourseLinearAlgebra2024,
  title = {A {{Course}} in {{Linear Algebra}}},
  author = {George, Raju K. and Ajayakumar, Abhijith},
  year = 2024,
  series = {University {{Texts}} in the {{Mathematical Sciences}}},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-99-8680-4},
  urldate = {2026-01-06},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-981-99-8679-8 978-981-99-8680-4},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/H4WMT2UM/George and Ajayakumar - 2024 - A Course in Linear Algebra.pdf}
}

@article{geuenichImpactsActiveSelfsupervised2024,
  title = {The Impacts of Active and Self-Supervised Learning on Efficient Annotation of Single-Cell Expression Data},
  author = {Geuenich, Michael J. and Gong, Dae-won and Campbell, Kieran R.},
  year = 2024,
  month = feb,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {1014},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-45198-y},
  urldate = {2026-01-31},
  abstract = {Abstract                            A crucial step in the analysis of single-cell data is annotating cells to cell types and states. While a myriad of approaches has been proposed, manual labeling of cells to create training datasets remains tedious and time-consuming. In the field of machine learning, active and self-supervised learning methods have been proposed to improve the performance of a classifier while reducing both annotation time and label budget. However, the benefits of such strategies for single-cell annotation have yet to be evaluated in realistic settings. Here, we perform a comprehensive benchmarking of active and self-supervised labeling strategies across a range of single-cell technologies and cell type annotation algorithms. We quantify the benefits of active learning and self-supervised strategies in the presence of cell type imbalance and variable similarity. We introduce adaptive reweighting, a heuristic procedure tailored to single-cell data---including a marker-aware version---that shows competitive performance with existing approaches. In addition, we demonstrate that having prior knowledge of cell type markers improves annotation accuracy. Finally, we summarize our findings into a set of recommendations for those implementing cell type annotation procedures or platforms. An R package implementing the heuristic approaches introduced in this work may be found at               https://github.com/camlab-bioml/leader               .},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/NSXU9D47/Geuenich et al. - 2024 - The impacts of active and self-supervised learning on efficient annotation of single-cell expression.pdf}
}

@article{goelKBAIEBOOKKNOWLEDGEBASED,
  title = {{{KBAI EBOOK}}: {{KNOWLEDGE-BASED ARTIFICIAL INTELLIGENCE}}},
  author = {Goel, Ashok and Joyner, David},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/WWCKMUVM/Goel and Joyner - KBAI EBOOK KNOWLEDGE-BASED ARTIFICIAL INTELLIGENCE.pdf}
}

@misc{guptaTopologyAwareUncertaintyImage2023,
  title = {Topology-{{Aware Uncertainty}} for {{Image Segmentation}}},
  author = {Gupta, Saumya and Zhang, Yikai and Hu, Xiaoling and Prasanna, Prateek and Chen, Chao},
  year = 2023,
  month = oct,
  number = {arXiv:2306.05671},
  eprint = {2306.05671},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.05671},
  urldate = {2026-01-07},
  abstract = {Segmentation of curvilinear structures such as vasculature and road networks is challenging due to relatively weak signals and complex geometry/topology. To facilitate and accelerate large scale annotation, one has to adopt semi-automatic approaches such as proofreading by experts. In this work, we focus on uncertainty estimation for such tasks, so that highly uncertain, and thus error-prone structures can be identified for human annotators to verify. Unlike most existing works, which provide pixel-wise uncertainty maps, we stipulate it is crucial to estimate uncertainty in the units of topological structures, e.g., small pieces of connections and branches. To achieve this, we leverage tools from topological data analysis, specifically discrete Morse theory (DMT), to first capture the structures, and then reason about their uncertainties. To model the uncertainty, we (1) propose a joint prediction model that estimates the uncertainty of a structure while taking the neighboring structures into consideration (inter-structural uncertainty); (2) propose a novel Probabilistic DMT to model the inherent uncertainty within each structure (intra-structural uncertainty) by sampling its representations via a perturb-and-walk scheme. On various 2D and 3D datasets, our method produces better structure-wise uncertainty maps compared to existing works. Code available at https://github.com/Saumya-Gupta-26/struct-uncertainty},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/weixiao09/Zotero/storage/2AHV5XP2/Gupta et al. - 2023 - Topology-Aware Uncertainty for Image Segmentation.pdf}
}

@misc{hadjadjPoolBasedActiveLearning2023,
  title = {Pool-{{Based Active Learning}} with {{Proper Topological Regions}}},
  author = {Hadjadj, Lies and Devijver, Emilie and Molinier, Remi and Amini, Massih-Reza},
  year = 2023,
  month = oct,
  number = {arXiv:2310.01597},
  eprint = {2310.01597},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.01597},
  urldate = {2026-01-07},
  abstract = {Machine learning methods usually rely on large sample size to have good performance, while it is difficult to provide labeled set in many applications. Pool-based active learning methods are there to detect, among a set of unlabeled data, the ones that are the most relevant for the training. We propose in this paper a meta-approach for pool-based active learning strategies in the context of multi-class classification tasks based on Proper Topological Regions. PTR, based on topological data analysis (TDA), are relevant regions used to sample cold-start points or within the active learning scheme. The proposed method is illustrated empirically on various benchmark datasets, being competitive to the classical methods from the literature.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology},
  file = {/Users/weixiao09/Zotero/storage/73FSDJYQ/Hadjadj et al. - 2023 - Pool-Based Active Learning with Proper Topological Regions.pdf}
}

@book{hidaryQuantumComputingApplied2019,
  title = {Quantum {{Computing}}: {{An Applied Approach}}},
  shorttitle = {Quantum {{Computing}}},
  author = {Hidary, Jack D.},
  year = 2019,
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-23922-0},
  urldate = {2026-01-01},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-030-23921-3 978-3-030-23922-0},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/HEGUIVQV/Hidary - 2019 - Quantum Computing An Applied Approach.pdf}
}

@book{hirschDifferentialTopology1976,
  title = {Differential Topology},
  author = {Hirsch, Morris W.},
  year = 1976,
  series = {Graduate Texts in Mathematics},
  number = {33},
  publisher = {Springer-Verlag},
  address = {New York Heidelberg Berlin},
  doi = {10.1007/978-1-4684-9449-5},
  abstract = {Intro -- Graduate Texts in Mathematics 33 -- Differential Topology -- Copyright -- Preface -- Table of Contents -- Introduction -- Chapter 1 Manifolds and Maps -- Chapter 2 Function Spaces -- Chapter 3 Transversality -- Chapter 4 Vector Bundles and Tubular Neighborhoods -- Chapter 5 Degrees, Intersection Numbers, and the Euler Characteristic -- Chapter 6 Morse Theory -- Chapter 7 Cobordism -- Chapter 8 Isotopy -- Chapter 9 Surfaces -- Bibliography -- Appendix -- Index},
  isbn = {978-1-4684-9449-5},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/XP7RKJFT/Hirsch - 1976 - Differential topology.pdf}
}

@book{hoffFirstCourseBayesian2009,
  title = {A {{First Course}} in {{Bayesian Statistical Methods}}},
  author = {Hoff, Peter D.},
  year = 2009,
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-0-387-92407-6},
  urldate = {2026-01-01},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-0-387-92299-7 978-0-387-92407-6},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/WE7SALQ7/Hoff - 2009 - A First Course in Bayesian Statistical Methods.pdf}
}

@misc{isufiTopologicalSignalProcessing2025,
  title = {Topological {{Signal Processing}} and {{Learning}}: {{Recent Advances}} and {{Future Challenges}}},
  shorttitle = {Topological {{Signal Processing}} and {{Learning}}},
  author = {Isufi, Elvin and Leus, Geert and {Beferull-Lozano}, Baltasar and Barbarossa, Sergio and Lorenzo, Paolo Di},
  year = 2025,
  month = feb,
  number = {arXiv:2412.01576},
  eprint = {2412.01576},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.01576},
  urldate = {2025-12-24},
  abstract = {Developing methods to process irregularly structured data is crucial in applications like gene-regulatory, brain, power, and socioeconomic networks. Graphs have been the go-to algebraic tool for modeling the structure via nodes and edges capturing their interactions, leading to the establishment of the fields of graph signal processing (GSP) and graph machine learning (GML). Key graph-aware methods include Fourier transform, filtering, sampling, as well as topology identification and spatiotemporal processing. Although versatile, graphs can model only pairwise dependencies in the data. To this end, topological structures such as simplicial and cell complexes have emerged as algebraic representations for more intricate structure modeling in data-driven systems, fueling the rapid development of novel topological-based processing and learning methods. This paper first presents the core principles of topological signal processing through the Hodge theory, a framework instrumental in propelling the field forward thanks to principled connections with GSP-GML. It then outlines advances in topological signal representation, filtering, and sampling, as well as inferring topological structures from data, processing spatiotemporal topological signals, and connections with topological machine learning. The impact of topological signal processing and learning is finally highlighted in applications dealing with flow data over networks, geometric processing, statistical ranking, biology, and semantic communication.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Electrical Engineering and Systems Science - Signal Processing},
  file = {/Users/weixiao09/Zotero/storage/9TYIIPCT/Isufi et al. - 2025 - Topological Signal Processing and Learning Recent Advances and Future Challenges.pdf}
}

@misc{jiMemoryAwareUncertaintyGuidedRetrieval2025,
  title = {Memory-{{Aware}} and {{Uncertainty-Guided Retrieval}} for {{Multi-Hop Question Answering}}},
  author = {Ji, Yuelyu and Meng, Rui and Li, Zhuochun and He, Daqing},
  year = 2025,
  month = mar,
  number = {arXiv:2503.23095},
  eprint = {2503.23095},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.23095},
  urldate = {2026-01-07},
  abstract = {Multi-hop question answering (QA) requires models to retrieve and reason over multiple pieces of evidence. While Retrieval-Augmented Generation (RAG) has made progress in this area, existing methods often suffer from two key limitations: (1) fixed or overly frequent retrieval steps, and (2) ineffective use of previously retrieved knowledge. We propose MIND (Memory-Informed and INteractive Dynamic RAG), a framework that addresses these challenges through: (i) prompt-based entity extraction to identify reasoning-relevant elements, (ii) dynamic retrieval triggering based on token-level entropy and attention signals, and (iii) memory-aware filtering, which stores high-confidence facts across reasoning steps to enable consistent multi-hop generation.https://github.com/JoyDajunSpaceCraft/ MIND.git.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/weixiao09/Zotero/storage/47GISPPD/Ji et al. - 2025 - Memory-Aware and Uncertainty-Guided Retrieval for Multi-Hop Question Answering.pdf}
}

@article{jospinHandsonBayesianNeural2022,
  title = {Hands-on {{Bayesian Neural Networks}} -- a {{Tutorial}} for {{Deep Learning Users}}},
  author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
  year = 2022,
  month = may,
  journal = {IEEE Computational Intelligence Magazine},
  volume = {17},
  number = {2},
  eprint = {2007.06823},
  primaryclass = {cs},
  pages = {29--48},
  issn = {1556-603X, 1556-6048},
  doi = {10.1109/MCI.2022.3155327},
  urldate = {2026-01-07},
  abstract = {Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides deep learning practitioners with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks, i.e., stochastic artificial neural networks trained using Bayesian methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/weixiao09/Zotero/storage/7CX3RXWZ/Jospin et al. - 2022 - Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users.pdf}
}

@article{kupersLecturesDifferentialTopology,
  title = {Lectures on Differential Topology},
  author = {Kupers, Alexander},
  abstract = {These are collected lecture notes on differential topology. Starting from the definitions, we discuss the foundational geometric results on smooth manifold. We also give an introduction to intersection theory, de Rham cohomology, and Morse theory.},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/GVPYIG6F/Kupers - Lectures on diﬀerential topology.pdf}
}

@article{linActiveLearningApproach2022,
  title = {An Active Learning Approach for Clustering Single-Cell {{RNA-seq}} Data},
  author = {Lin, Xiang and Liu, Haoran and Wei, Zhi and Roy, Senjuti Basu and Gao, Nan},
  year = 2022,
  month = mar,
  journal = {Laboratory Investigation},
  volume = {102},
  number = {3},
  pages = {227--235},
  issn = {00236837},
  doi = {10.1038/s41374-021-00639-w},
  urldate = {2026-01-31},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/VMYESNR3/Lin et al. - 2022 - An active learning approach for clustering single-cell RNA-seq data.pdf}
}

@misc{liuSimplePrincipledUncertainty2020,
  title = {Simple and {{Principled Uncertainty Estimation}} with {{Deterministic Deep Learning}} via {{Distance Awareness}}},
  author = {Liu, Jeremiah Zhe and Lin, Zi and Padhy, Shreyas and Tran, Dustin and {Bedrax-Weiss}, Tania and Lakshminarayanan, Balaji},
  year = 2020,
  month = oct,
  number = {arXiv:2006.10108},
  eprint = {2006.10108},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.10108},
  urldate = {2026-01-07},
  abstract = {Bayesian neural networks (BNN) and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model. However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify input distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data in the input space, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer with a Gaussian process. On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/weixiao09/Zotero/storage/QXKRA8FQ/Liu et al. - 2020 - Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness.pdf}
}

@misc{meilaManifoldLearningWhat2023,
  title = {Manifold Learning: What, How, and Why},
  shorttitle = {Manifold Learning},
  author = {Meil{\u a}, Marina and Zhang, Hanyu},
  year = 2023,
  month = nov,
  number = {arXiv:2311.03757},
  eprint = {2311.03757},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.03757},
  urldate = {2025-12-24},
  abstract = {Manifold learning (ML), known also as non-linear dimension reduction, is a set of methods to find the low dimensional structure of data. Dimension reduction for large, high dimensional data is not merely a way to reduce the data; the new representations and descriptors obtained by ML reveal the geometric shape of high dimensional point clouds, and allow one to visualize, denoise and interpret them. This survey presents the principles underlying ML, the representative methods, as well as their statistical foundations from a practicing statistician's perspective. It describes the trade-offs, and what theory tells us about the parameter and algorithmic choices we make in order to obtain reliable conclusions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/weixiao09/Zotero/storage/9NI36TIS/Meilă and Zhang - 2023 - Manifold learning what, how, and why.pdf}
}

@misc{moorTopologicalAutoencoders2021,
  title = {Topological {{Autoencoders}}},
  author = {Moor, Michael and Horn, Max and Rieck, Bastian and Borgwardt, Karsten},
  year = 2021,
  month = may,
  number = {arXiv:1906.00722},
  eprint = {1906.00722},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.00722},
  urldate = {2026-01-07},
  abstract = {We propose a novel approach for preserving topological structures of the input space in latent representations of autoencoders. Using persistent homology, a technique from topological data analysis, we calculate topological signatures of both the input and latent space to derive a topological loss term. Under weak theoretical assumptions, we construct this loss in a differentiable manner, such that the encoding learns to retain multi-scale connectivity information. We show that our approach is theoretically well-founded and that it exhibits favourable latent representations on a synthetic manifold as well as on real-world image data sets, while preserving low reconstruction errors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  file = {/Users/weixiao09/Zotero/storage/RQKKEGTF/Moor et al. - 2021 - Topological Autoencoders.pdf}
}

@book{murphyProbabilisticMachineLearning2023,
  title = {Probabilistic {{Machine Learning}}: {{Advanced Topics}}},
  shorttitle = {Probabilistic {{Machine Learning}}},
  author = {Murphy, Kevin P.},
  year = 2023,
  series = {Adaptive {{Computation}} and {{Machine Learning Series}}},
  edition = {1st ed},
  publisher = {MIT Press},
  address = {Cambridge},
  abstract = {Intro -- Copyright -- Preface -- 1 Introduction -- I Fundamentals -- 2 Probability -- 2.1 Introduction -- 2.1.1 Probability space -- 2.1.2 Discrete random variables -- 2.1.3 Continuous random variables -- 2.1.4 Probability axioms -- 2.1.5 Conditional probability -- 2.1.6 Bayes' rule -- 2.2 Some common probability distributions -- 2.2.1 Discrete distributions -- 2.2.2 Continuous distributions on {$\mathbb{R}$} -- 2.2.3 Continuous distributions on {$\mathbb{R}$}+ -- 2.2.4 Continuous distributions on [0, 1] -- 2.2.5 Multivariate continuous distributions -- 2.3 Gaussian joint distributions -- 2.3.1 The multivariate normal -- 2.3.2 Linear Gaussian systems -- 2.3.3 A general calculus for linear Gaussian systems -- 2.4 The exponential family -- 2.4.1 Definition -- 2.4.2 Examples -- 2.4.3 Log partition function is cumulant generating function -- 2.4.4 Canonical (natural) vs mean (moment) parameters -- 2.4.5 MLE for the exponential family -- 2.4.6 Exponential dispersion family -- 2.4.7 Maximum entropy derivation of the exponential family -- 2.5 Transformations of random variables -- 2.5.1 Invertible transformations (bijections) -- 2.5.2 Monte Carlo approximation -- 2.5.3 Probability integral transform -- 2.6 Markov chains -- 2.6.1 Parameterization -- 2.6.2 Application: language modeling -- 2.6.3 Parameter estimation -- 2.6.4 Stationary distribution of a Markov chain -- 2.7 Divergence measures between probability distributions -- 2.7.1 f-divergence -- 2.7.2 Integral probability metrics -- 2.7.3 Maximum mean discrepancy (MMD) -- 2.7.4 Total variation distance -- 2.7.5 Density ratio estimation using binary classifiers -- 3 Statistics -- 3.1 Introduction -- 3.2 Bayesian statistics -- 3.2.1 Tossing coins -- 3.2.2 Modeling more complex data -- 3.2.3 Selecting the prior -- 3.2.4 Computational issues -- 3.2.5 Exchangeability and de Finetti's theorem -- 3.3 Frequentist statistics},
  isbn = {978-0-262-37600-6},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/J4P4GSPY/Murphy - 2023 - Probabilistic Machine Learning Advanced Topics.pdf}
}

@article{paulTranscriptionalHeterogeneityLineage2015,
  title = {Transcriptional {{Heterogeneity}} and {{Lineage Commitment}} in {{Myeloid Progenitors}}},
  author = {Paul, Franziska and Arkin, Ya'ara and Giladi, Amir and Jaitin, Diego~Adhemar and Kenigsberg, Ephraim and {Keren-Shaul}, Hadas and Winter, Deborah and {Lara-Astiaso}, David and Gury, Meital and Weiner, Assaf and David, Eyal and Cohen, Nadav and Lauridsen, Felicia~Kathrine~Bratt and Haas, Simon and Schlitzer, Andreas and Mildner, Alexander and Ginhoux, Florent and Jung, Steffen and Trumpp, Andreas and Porse, Bo~Torben and Tanay, Amos and Amit, Ido},
  year = 2015,
  month = dec,
  journal = {Cell},
  volume = {163},
  number = {7},
  pages = {1663--1677},
  issn = {00928674},
  doi = {10.1016/j.cell.2015.11.013},
  urldate = {2026-01-31},
  abstract = {Within the bone marrow, stem cells differentiate and give rise to diverse blood cell types and functions. Currently, hematopoietic progenitors are defined using surface markers combined with functional assays that are not directly linked with in vivo differentiation potential or gene regulatory mechanisms. Here, we comprehensively map myeloid progenitor subpopulations by transcriptional sorting of single cells from the bone marrow. We describe multiple progenitor subgroups, showing unexpected transcriptional priming toward seven differentiation fates but no progenitors with a mixed state. Transcriptional differentiation is correlated with combinations of known and previously undefined transcription factors, suggesting that the process is tightly regulated. Histone maps and knockout assays are consistent with early transcriptional priming, while traditional transplantation experiments suggest that in vivo priming may still allow for plasticity given strong perturbations. These data establish a reference model and general framework for studying hematopoiesis at single-cell resolution.},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/C8VIT6WR/Paul et al. - 2015 - Transcriptional Heterogeneity and Lineage Commitment in Myeloid Progenitors.pdf}
}

@article{qiuReversedGraphEmbedding2017,
  title = {Reversed Graph Embedding Resolves Complex Single-Cell Trajectories},
  author = {Qiu, Xiaojie and Mao, Qi and Tang, Ying and Wang, Li and Chawla, Raghav and Pliner, Hannah A and Trapnell, Cole},
  year = 2017,
  month = oct,
  journal = {Nature Methods},
  volume = {14},
  number = {10},
  pages = {979--982},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.4402},
  urldate = {2026-01-31},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/FQ8GLQ8T/Qiu et al. - 2017 - Reversed graph embedding resolves complex single-cell trajectories.pdf}
}

@incollection{rasmussenGaussianProcessesMachine2004,
  title = {Gaussian {{Processes}} in {{Machine Learning}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}},
  author = {Rasmussen, Carl Edward},
  editor = {Bousquet, Olivier and Von Luxburg, Ulrike and R{\"a}tsch, Gunnar},
  year = 2004,
  volume = {3176},
  pages = {63--71},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-28650-9_4},
  urldate = {2026-01-07},
  abstract = {We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-540-23122-6 978-3-540-28650-9},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/T636AYXU/Rasmussen - 2004 - Gaussian Processes in Machine Learning.pdf}
}

@book{rasmussenGaussianProcessesMachine2008,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = 2008,
  series = {Adaptive Computation and Machine Learning},
  edition = {3. print},
  publisher = {MIT Press},
  address = {Cambridge, Mass.},
  isbn = {978-0-262-18253-9},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/MDR2FPST/Rasmussen and Williams - 2008 - Gaussian processes for machine learning.pdf}
}

@article{rizviSinglecellTopologicalRNAseq2017,
  title = {Single-Cell Topological {{RNA-seq}} Analysis Reveals Insights into Cellular Differentiation and Development},
  author = {Rizvi, Abbas H and Camara, Pablo G and Kandror, Elena K and Roberts, Thomas J and Schieren, Ira and Maniatis, Tom and Rabadan, Raul},
  year = 2017,
  month = jun,
  journal = {Nature Biotechnology},
  volume = {35},
  number = {6},
  pages = {551--560},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt.3854},
  urldate = {2026-01-31},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/V57LIHRM/Rizvi et al. - 2017 - Single-cell topological RNA-seq analysis reveals insights into cellular differentiation and developm.pdf}
}

@book{robertBayesianChoiceDecisionTheoretic2007,
  title = {The {{Bayesian Choice}}: {{From Decision-Theoretic Foundations}} to {{Computational Implementation}}},
  shorttitle = {The {{Bayesian Choice}}},
  editor = {Robert, Christian P.},
  year = 2007,
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/0-387-71599-1},
  isbn = {978-0-387-71598-8 978-0-387-71599-5},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/8WW835FE/Robert - 2007 - The Bayesian Choice From Decision-Theoretic Foundations to Computational Implementation.pdf}
}

@book{romanAdvancedLinearAlgebra2008,
  title = {Advanced {{Linear Algebra}}},
  author = {Roman, Steven},
  year = 2008,
  series = {Graduate Texts in Mathematics},
  edition = {3. ed},
  number = {135},
  publisher = {Springer},
  address = {New York, NY},
  isbn = {978-0-387-72828-5 978-1-4419-2498-8},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/K7KNF6GC/Roman - 2008 - Advanced Linear Algebra.pdf}
}

@misc{sinhaVariationalAdversarialActive2019,
  title = {Variational {{Adversarial Active Learning}}},
  author = {Sinha, Samarth and Ebrahimi, Sayna and Darrell, Trevor},
  year = 2019,
  month = oct,
  number = {arXiv:1904.00370},
  eprint = {1904.00370},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.00370},
  urldate = {2026-01-07},
  abstract = {Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Unlike conventional active learning algorithms, our approach is task agnostic, i.e., it does not depend on the performance of the task for which we are trying to acquire labeled data. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled and labeled data. The mini-max game between the VAE and the adversarial network is played such that while the VAE tries to trick the adversarial network into predicting that all data points are from the labeled pool, the adversarial network learns how to discriminate between dissimilarities in the latent space. We extensively evaluate our method on various image classification and semantic segmentation benchmark datasets and establish a new state of the art on \$\textbackslash text\textbraceleft CIFAR10/100\textbraceright\$, \$\textbackslash text\textbraceleft Caltech-256\textbraceright\$, \$\textbackslash text\textbraceleft ImageNet\textbraceright\$, \$\textbackslash text\textbraceleft Cityscapes\textbraceright\$, and \$\textbackslash text\textbraceleft BDD100K\textbraceright\$. Our results demonstrate that our adversarial approach learns an effective low dimensional latent space in large-scale settings and provides for a computationally efficient sampling method. Our code is available at https://github.com/sinhasam/vaal.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/weixiao09/Zotero/storage/J97F2R6G/Sinha et al. - 2019 - Variational Adversarial Active Learning.pdf}
}

@article{tenenbaumGlobalGeometricFramework2000,
  title = {A {{Global Geometric Framework}} for {{Nonlinear Dimensionality Reduction}}},
  author = {Tenenbaum, Joshua B. and Silva, Vin De and Langford, John C.},
  year = 2000,
  month = dec,
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2319--2323},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.290.5500.2319},
  urldate = {2025-12-24},
  abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs---30,000 auditory nerve fibers or 10               6               optic nerve fibers---a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/BADQLQZ5/Tenenbaum et al. - 2000 - A Global Geometric Framework for Nonlinear Dimensionality Reduction.pdf}
}

@article{tenenbaumGlobalGeometricFramework2000a,
  title = {A {{Global Geometric Framework}} for {{Nonlinear Dimensionality Reduction}}},
  author = {Tenenbaum, Joshua B. and Silva, Vin De and Langford, John C.},
  year = 2000,
  month = dec,
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2319--2323},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.290.5500.2319},
  urldate = {2025-12-24},
  abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs---30,000 auditory nerve fibers or 10               6               optic nerve fibers---a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/E7AAC4GD/Tenenbaum et al. - 2000 - A Global Geometric Framework for Nonlinear Dimensionality Reduction.pdf}
}

@book{tuIntroductionManifolds2011,
  title = {An {{Introduction}} to {{Manifolds}}},
  author = {Tu, Loring W.},
  year = 2011,
  series = {Universitext},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4419-7400-6},
  urldate = {2026-01-01},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-1-4419-7399-3 978-1-4419-7400-6},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/3ASURGXM/Tu - 2011 - An Introduction to Manifolds.pdf}
}

@article{zomorodianComputingPersistentHomology,
  title = {Computing {{Persistent Homology}}},
  author = {Zomorodian, Afra},
  abstract = {We show that the persistent homology of a filtered ddimensional simplicial complex is simply the standard homology of a particular graded module over a polynomial ring. Our analysis establishes the existence of a simple description of persistent homology groups over arbitrary fields. It also enables us to derive a natural algorithm for computing persistent homology of spaces in arbitrary dimension over any field. This result generalizes and extends the previously known algorithm that was restricted to subcomplexes of S3 and Z2 coefficients. Finally, our study implies the lack of a simple classification over non-fields. Instead, we give an algorithm for computing individual persistent homology groups over an arbitrary principal ideal domains in any dimension.},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/RDIB46HV/Zomorodian - Computing Persistent Homology.pdf}
}

@article{zomorodianComputingPersistentHomology2005,
  title = {Computing {{Persistent Homology}}},
  author = {Zomorodian, Afra and Carlsson, Gunnar},
  year = 2005,
  month = feb,
  journal = {Discrete \& Computational Geometry},
  volume = {33},
  number = {2},
  pages = {249--274},
  issn = {0179-5376, 1432-0444},
  doi = {10.1007/s00454-004-1146-y},
  urldate = {2026-01-31},
  abstract = {We show that the persistent homology of a filtered d-dimensional simplicial complex is simply the standard homology of a particular graded module over a polynomial ring. Our analysis establishes the existence of a simple description of persistent homology groups over arbitrary fields. It also enables us to derive a natural algorithm for computing persistent homology of spaces in arbitrary dimension over any field. This result generalizes and extends the previously known algorithm that was restricted to subcomplexes of S3 and Z2 coefficients. Finally, our study implies the lack of a simple classification over non-fields. Instead, we give an algorithm for computing individual persistent homology groups over an arbitrary principal ideal domain in any dimension.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/weixiao09/Zotero/storage/6AL3J2IP/Zomorodian and Carlsson - 2005 - Computing Persistent Homology.pdf}
}
